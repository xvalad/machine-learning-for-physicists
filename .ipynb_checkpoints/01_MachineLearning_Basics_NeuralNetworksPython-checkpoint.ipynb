{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with Pure Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 1\n",
    "\n",
    "See https://machine-learning-for-physicists.org and the current course website linked there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to:\n",
    "- implement the forward-pass (evaluation) of a deep, fully connected neural network in a few lines of python\n",
    "- do that efficiently using batches\n",
    "- illustrate the results for randomly initialized neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the \"numpy\" library for linear algebra\n",
    "\n",
    "# In the lecture videos, I do this:\n",
    "#\n",
    "# from numpy import *\n",
    "#\n",
    "# WARNING: It is generally considered bad\n",
    "# programming style to \"import *\", as it\n",
    "# can lead to confusion. For me, I\n",
    "#  (1) ALWAYS import numpy\n",
    "#  (2) NEVER import any other package in this * way\n",
    "# Therefore, there is never confusion for me, and\n",
    "# it makes my code a bit more readable (for me).\n",
    "# However, since 99% of people are using the \n",
    "# syntax \"import numpy as np\" and then\n",
    "# access \"np.exp()\" etc., you\n",
    "# should probably also use \"np\" once you start\n",
    "# exchanging code with others. I convert\n",
    "# back to the np. syntax when I turn my\n",
    "# converged code into a module.\n",
    "#\n",
    "# It is apparently officially accepted to explicitly\n",
    "# list all the functions you need from numpy:\n",
    "\n",
    "from numpy import array, zeros, exp, random, dot, shape, reshape, meshgrid, linspace\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very simple neural network (no hidden layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network with N0 input neurons and N1 output neurons (no hidden layer)\n",
    "\n",
    "$$y^{\\rm out}_j = f(\\sum_k w_{jk} y^{\\rm in}_k + b_j)$$\n",
    "\n",
    "where $w$ is the weight matrix, $b$ is the bias vector, and $f$ would be the activation function (e.g. the sigmoid here), which is applied independently for each $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0=3 # input layer size\n",
    "N1=2 # output layer size\n",
    "\n",
    "# initialize random weights: array dimensions N1xN0\n",
    "w=random.uniform(low=-1,high=+1,size=(N1,N0))\n",
    "# initialize random biases: N1 vector\n",
    "b=random.uniform(low=-1,high=+1,size=N1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input values\n",
    "y_in=array([0.2,0.4,-0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate network by hand, in two steps\n",
    "z=dot(w,y_in)+b # result: the vector of 'z' values, length N1\n",
    "y_out=1/(1+exp(-z)) # the 'sigmoid' function (applied elementwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"network input y_in:\", y_in)\n",
    "print(\"weights w:\", w)\n",
    "print(\"bias vector b:\", b)\n",
    "print(\"linear superposition z:\", z)\n",
    "print(\"network output y_out:\", y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize network result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still stay with the simple network, but define a function that evaluates the network, and visualize the  output for various inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that applies the network\n",
    "def apply_net(y_in):\n",
    "    global w, b\n",
    "    \n",
    "    z=dot(w,y_in)+b    \n",
    "    return(1/(1+exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0=2 # input layer size\n",
    "N1=1 # output layer size\n",
    "\n",
    "w=random.uniform(low=-10,high=+10,size=(N1,N0)) # random weights: N1xN0\n",
    "b=random.uniform(low=-1,high=+1,size=N1) # biases: N1 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_net([0.8,0.3]) # a simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is NOT the most efficient way to do this! (but simple)\n",
    "# We will later learn to use array syntax efficiently\n",
    "\n",
    "M=50 # will create picture of size MxM\n",
    "y_out=zeros([M,M]) # array MxM, to hold the result\n",
    "\n",
    "for j1 in range(M):\n",
    "    for j2 in range(M):\n",
    "        # out of these integer indices, generate\n",
    "        # two values in the range -0.5...0.5\n",
    "        # and then apply the network to those two\n",
    "        # input values\n",
    "        value0=float(j1)/M-0.5\n",
    "        value1=float(j2)/M-0.5\n",
    "        y_out[j1,j2]=apply_net([value0,value1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display image\n",
    "plt.imshow(y_out,origin='lower',extent=(-0.5,0.5,-0.5,0.5))\n",
    "plt.colorbar()\n",
    "plt.title(\"NN output as a function of input values\")\n",
    "plt.xlabel(\"y_2\")\n",
    "plt.ylabel(\"y_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now visualize a network with one hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to have multiple weight matrices (for each pair of subsequent layers there is one weight matrix). The function that \"applies a layer\", i.e. goes from one layer to the next, is essentially the same as the function evaluating the simple network above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that evaluates one layer based\n",
    "# on the neuron values in the preceding layer\n",
    "def apply_layer(y_in,w,b): \n",
    "    z=dot(w,y_in)+b\n",
    "    return(1/(1+exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0=2 # input layer size\n",
    "N1=30 # hidden layer size\n",
    "N2=1 # output layer size\n",
    "\n",
    "# weights and biases\n",
    "# from input layer to hidden layer:\n",
    "w1=random.uniform(low=-10,high=+10,size=(N1,N0)) # random weights: N1xN0\n",
    "b1=random.uniform(low=-1,high=+1,size=N1) # biases: N1 vector\n",
    "\n",
    "# weights+biases from hidden layer to output layer:\n",
    "w2=random.uniform(low=-10,high=+10,size=(N2,N1)) # random weights\n",
    "b2=random.uniform(low=-1,high=+1,size=N2) # biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network by subsequently\n",
    "# evaluating the two steps (input to hidden and\n",
    "# hidden to output)\n",
    "def apply_net(y_in):\n",
    "    global w1,b1,w2,b2\n",
    "    \n",
    "    y1=apply_layer(y_in,w1,b1)\n",
    "    y2=apply_layer(y1,w2,b2)\n",
    "    return(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, obtain values for a range of inputs\n",
    "# Note: this is NOT the most efficient way to do this! (but simple)\n",
    "\n",
    "M=50 # will create picture of size MxM\n",
    "y_out=zeros([M,M]) # array MxM, to hold the result\n",
    "\n",
    "for j1 in range(M):\n",
    "    for j2 in range(M):\n",
    "        value0=float(j1)/M-0.5\n",
    "        value1=float(j2)/M-0.5\n",
    "        y_out[j1,j2]=apply_net([value0,value1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display image\n",
    "plt.imshow(y_out,origin='lower',extent=(-0.5,0.5,-0.5,0.5))\n",
    "plt.colorbar()\n",
    "plt.title(\"NN output as a function of input values\")\n",
    "plt.xlabel(\"y_2\")\n",
    "plt.ylabel(\"y_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the shape of the output is already more 'complex' than that of a simple network without hidden layer! Let's go further in that direction..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a network to a 'batch' of samples (python trickery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: apply network to many samples in parallel (no 'for' loops!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small excursion: matrix-vector multiplication and python broadcasting of array dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the dot product works: \n",
    "# 'contracts' (sums over) the innermost index\n",
    "W=zeros([7,8])\n",
    "y=zeros([8,30]) \n",
    "# here '30' would stand for the number of samples\n",
    "# in our envisaged network applications\n",
    "shape(dot(W,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try to add the bias vector entries,\n",
    "# in the most naive way (beware!)\n",
    "B=zeros(7)\n",
    "result=dot(W,y)+B # will produce an error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But with a re-ordering of indices, this works!\n",
    "# So, let's take the dimension of size 30 to be\n",
    "# the very first one:\n",
    "y=zeros([30,8])\n",
    "W=zeros([8,7])\n",
    "shape(dot(y,W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now try again adding the bias vector,\n",
    "# again in a naive way\n",
    "B=zeros(7)\n",
    "result=dot(y,W)+B \n",
    "# will give the desired result, \n",
    "# because B is 'broadcast' to shape (30,7)\n",
    "shape(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the functions that evaluate a layer and evaluate the network, with batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up for batch processing, i.e. parallel evaluation of many input samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_layer_new(y_in,w,b): # a function that applies a layer    \n",
    "    z=dot(y_in,w)+b # note different order in matrix product!\n",
    "    return(1/(1+exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_net_new(y_in): # same as before, but with new layer function\n",
    "    global w1,b1,w2,b2\n",
    "    \n",
    "    y1=apply_layer_new(y_in,w1,b1)\n",
    "    y2=apply_layer_new(y1,w2,b2)\n",
    "    return(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0=2 # input layer size\n",
    "N1=5 # hidden layer size\n",
    "N2=1 # output layer size\n",
    "\n",
    "# from input layer to hidden layer:\n",
    "w1=random.uniform(low=-10,high=+10,size=(N0,N1)) # NEW ORDER!! N0xN1\n",
    "b1=random.uniform(low=-1,high=+1,size=N1) # biases: N1 vector\n",
    "\n",
    "# from hidden layer to output layer:\n",
    "w2=random.uniform(low=-10,high=+10,size=(N1,N2)) # NEW ORDER N1xN2\n",
    "b2=random.uniform(low=-1,high=+1,size=N2) # biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=10000\n",
    "y=random.uniform(low=-1,high=1,size=(batchsize,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out=apply_net_new(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape(y_out) \n",
    "# these were 10000 samples evaluated in parallel!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now visualize multi-layer net again, but more efficiently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the pixels in the image are samples, process all of them together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=50\n",
    "# Generate a 'mesh grid', i.e. x,y values in an image\n",
    "v0,v1=meshgrid(linspace(-0.5,0.5,M),linspace(-0.5,0.5,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2)\n",
    "ax[0].imshow(v0,origin='lower')\n",
    "ax[0].set_title(\"input value v0\")\n",
    "ax[1].imshow(v1,origin='lower')\n",
    "ax[1].set_title(\"input value v1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0flat=v0.flatten() # make 1D array out of 2D array\n",
    "v1flat=v1.flatten() \n",
    "# that means: MxM matrix becomes M^2 vector\n",
    "shape(v0flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=shape(v0flat)[0] # number of samples = number of pixels\n",
    "y_in=zeros([batchsize,2])\n",
    "y_in[:,0]=v0flat # fill first component (index 0)\n",
    "y_in[:,1]=v1flat # fill second component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply net to all these samples simultaneously!\n",
    "y_out=apply_net_new(y_in) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape(y_out) # this is not a vector but a funny matrix batchsize x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn this back into a 2D matrix (image)\n",
    "y_2D=reshape(y_out[:,0],[M,M]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_2D,origin='lower')\n",
    "plt.title(\"NN output (one hidden layer)\")\n",
    "plt.xlabel(\"v0\")\n",
    "plt.ylabel(\"v1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For fun: a network with MANY hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nlayers=20 # not counting the input layer & the output layer\n",
    "LayerSize=100\n",
    "\n",
    "Weights=random.uniform(low=-3,high=3,size=[Nlayers,LayerSize,LayerSize])\n",
    "Biases=random.uniform(low=-1,high=1,size=[Nlayers,LayerSize])\n",
    "\n",
    "# for the first hidden layer (coming in from the input layer)\n",
    "WeightsFirst=random.uniform(low=-1,high=1,size=[2,LayerSize])\n",
    "BiasesFirst=random.uniform(low=-1,high=1,size=LayerSize)\n",
    "\n",
    "# for the final layer (i.e. the output neuron)\n",
    "WeightsFinal=random.uniform(low=-1,high=1,size=[LayerSize,1])\n",
    "BiasesFinal=random.uniform(low=-1,high=1,size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_multi_net(y_in):\n",
    "    global Weights, Biases, WeightsFinal, BiasesFinal, Nlayers\n",
    "    \n",
    "    y=apply_layer_new(y_in,WeightsFirst,BiasesFirst)    \n",
    "    for j in range(Nlayers):\n",
    "        y=apply_layer_new(y,Weights[j,:,:],Biases[j,:])\n",
    "    output=apply_layer_new(y,WeightsFinal,BiasesFinal)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 'mesh grid', i.e. x,y values in an image\n",
    "M=40\n",
    "v0,v1=meshgrid(linspace(-0.5,0.5,M),linspace(-0.5,0.5,M))\n",
    "batchsize=M**2 # number of samples = number of pixels = M^2\n",
    "y_in=zeros([batchsize,2])\n",
    "y_in[:,0]=v0.flatten() # fill first component (index 0)\n",
    "y_in[:,1]=v1.flatten() # fill second component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the MxM input grid that we generated above \n",
    "y_out=apply_multi_net(y_in) # apply net to all these samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2D=reshape(y_out[:,0],[M,M]) # back to 2D image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_2D,origin='lower',extent=[-0.5,0.5,-0.5,0.5],interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same, but high-resolution (400x400 picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=400\n",
    "# Generate a 'mesh grid', i.e. x,y values in an image\n",
    "v0,v1=meshgrid(linspace(-0.5,0.5,M),linspace(-0.5,0.5,M))\n",
    "batchsize=M**2 # number of samples = number of pixels = M^2\n",
    "y_in=zeros([batchsize,2])\n",
    "y_in[:,0]=v0.flatten() # fill first component (index 0)\n",
    "y_in[:,1]=v1.flatten() # fill second component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function takes a few seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the MxM input grid that we generated above \n",
    "y_out=apply_multi_net(y_in) # apply net to all these samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2D=reshape(y_out[:,0],[M,M]) # back to 2D image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.axes([0,0,1,1]) # fill all of the picture with the image\n",
    "plt.imshow(y_2D,origin='lower',extent=[-0.5,0.5,-0.5,0.5],interpolation='nearest')\n",
    "plt.axis('off') # no axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try another nonlinearity: rectified linear units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=linspace(-1,1,100)\n",
    "plt.plot(x,x*(x>0),linewidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_layer_new_relu(y_in,w,b): # a function that applies a layer    \n",
    "    z=dot(y_in,w)+b # note different order in matrix product!\n",
    "    return(z*(z>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_multi_net_relu(y_in):\n",
    "    global Weights, Biases, WeightsFinal, BiasesFinal, Nlayers\n",
    "    \n",
    "    y=apply_layer_new_relu(y_in,WeightsFirst,BiasesFirst)    \n",
    "    for j in range(Nlayers):\n",
    "        y=apply_layer_new_relu(y,Weights[j,:,:],Biases[j,:])\n",
    "    output=apply_layer_new_relu(y,WeightsFinal,BiasesFinal)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nlayers=20 # not counting the input layer & the output layer\n",
    "LayerSize=100\n",
    "\n",
    "Weights=random.uniform(low=-1,high=1,size=[Nlayers,LayerSize,LayerSize])\n",
    "Biases=0*random.uniform(low=-1,high=1,size=[Nlayers,LayerSize])\n",
    "\n",
    "# for the first hidden layer (coming in from the input layer)\n",
    "WeightsFirst=random.uniform(low=-1,high=1,size=[2,LayerSize])\n",
    "BiasesFirst=0*random.uniform(low=-1,high=1,size=LayerSize)\n",
    "\n",
    "# for the final layer (i.e. the output neuron)\n",
    "WeightsFinal=random.uniform(low=-1,high=1,size=[LayerSize,1])\n",
    "BiasesFinal=0*random.uniform(low=-1,high=1,size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=400\n",
    "# Generate a 'mesh grid', i.e. x,y values in an image\n",
    "v0,v1=meshgrid(linspace(-0.5,0.5,M),linspace(-0.5,0.5,M))\n",
    "batchsize=M**2 # number of samples = number of pixels = M^2\n",
    "y_in=zeros([batchsize,2])\n",
    "y_in[:,0]=v0.flatten() # fill first component (index 0)\n",
    "y_in[:,1]=v1.flatten() # fill second component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the MxM input grid that we generated above \n",
    "y_out=apply_multi_net_relu(y_in) # apply net to all these samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2D=reshape(y_out[:,0],[M,M]) # back to 2D image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.axes([0,0,1,1]) # fill all of the picture with the image\n",
    "plt.imshow(y_2D,origin='lower',extent=[-0.5,0.5,-0.5,0.5],interpolation='nearest')\n",
    "plt.axis('off') # no axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now zoom out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=400\n",
    "# Generate a 'mesh grid', i.e. x,y values in an image\n",
    "v0,v1=meshgrid(linspace(-10,10,M),linspace(-10,10,M))\n",
    "batchsize=M**2 # number of samples = number of pixels = M^2\n",
    "y_in=zeros([batchsize,2])\n",
    "y_in[:,0]=v0.flatten() # fill first component (index 0)\n",
    "y_in[:,1]=v1.flatten() # fill second component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the MxM input grid that we generated above \n",
    "y_out=apply_multi_net_relu(y_in) # apply net to all these samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2D=reshape(y_out[:,0],[M,M]) # back to 2D image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.axes([0,0,1,1]) # fill all of the picture with the image\n",
    "plt.imshow(y_2D,origin='lower',extent=[-0.5,0.5,-0.5,0.5],interpolation='nearest')\n",
    "plt.axis('off') # no axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_2D[:,200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
