{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear curve fitting by stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 1, Tutorials\n",
    "\n",
    "Go to https://machine-learning-for-physicists.org and follow the link to the current course website there (the website offers code like this for download)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how stochastic gradient descent can help fit an arbitrary function (neural networks essentially do the same, but in much higher dimensions and with many more parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy import array, zeros, exp, random, dot, shape, reshape, meshgrid, linspace\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parametrized nonlinear function\n",
    "def f(theta,x):\n",
    "    \"\"\"\n",
    "    theta are the parameters\n",
    "    x are the input values (can be an array)\n",
    "    \"\"\"\n",
    "    return(theta[0]/((x-theta[1])**2+1.0))\n",
    "\n",
    "# Define the gradient of the parametrized function\n",
    "# with respect to its parameters\n",
    "def f_grad(theta,x):\n",
    "    \"\"\"\n",
    "    Return the gradient of f with respect to theta\n",
    "    shape [n_theta,n_samples]\n",
    "    where n_theta=len(theta)\n",
    "    and n_samples=len(x)\n",
    "    \"\"\"\n",
    "    return(np.array([\n",
    "        1./((x-theta[1])**2+1.0)\n",
    "    ,\n",
    "        2*(x-theta[1])*theta[0]/((x-theta[1])**2+1.0)\n",
    "    ]\n",
    "    ))\n",
    "\n",
    "# Define the actual function (the target, to be fitted)\n",
    "def true_f(x):\n",
    "    return( 3.0/((x-0.5)**2+1.0) )\n",
    "\n",
    "# Get randomly sampled x values\n",
    "def samples(nsamples,width=2.0):\n",
    "    return(width*np.random.randn(nsamples))\n",
    "\n",
    "# Get the average cost function on a grid of 2 parameters\n",
    "def get_avg_cost(theta0s,theta1s,nsamples):\n",
    "    n0=len(theta0s)\n",
    "    n1=len(theta1s)\n",
    "    C=np.zeros([n0,n1])\n",
    "    for j0 in range(n0):\n",
    "        for j1 in range(n1):\n",
    "            theta=np.array([theta0s[j0],theta1s[j1]])\n",
    "            x=samples(nsamples)\n",
    "            C[j0,j1]=0.5*np.average((f(theta,x)-true_f(x))**2)\n",
    "    return(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take arbitrary parameters as starting point\n",
    "theta=np.array([1.5,-2.3])\n",
    "\n",
    "x=samples(100)\n",
    "# illustrate the parametrized function, at sampled points,\n",
    "# compare against actual function\n",
    "plt.scatter(x,f(theta,x),color=\"orange\")\n",
    "plt.scatter(x,true_f(x),color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average cost function:\n",
    "theta0s=np.linspace(-3,6,40)\n",
    "theta1s=np.linspace(-2,3,40)\n",
    "C=get_avg_cost(theta0s,theta1s,10000)\n",
    "nlevels=20\n",
    "X,Y=np.meshgrid(theta0s,theta1s,indexing='ij')\n",
    "plt.contourf(X,Y,C,nlevels)\n",
    "plt.contour(X,Y,C,nlevels,colors=\"white\")\n",
    "plt.xlabel(\"theta_0\")\n",
    "plt.ylabel(\"theta_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: do gradient descent and, for each step,\n",
    "# plot the (sampled) true function vs. the parametrized function\n",
    "# Also plot the current location of parameters theta\n",
    "# (over the average cost function)\n",
    "\n",
    "# import functions for updating display \n",
    "# (simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "# take arbitrary parameters as starting point\n",
    "theta=np.array([-1.0,2.0])\n",
    "\n",
    "# do many steps of stochastic gradient descent,\n",
    "# continue showing the comparison!\n",
    "eta=.2 # \"learning rate\" (gradient descent step size)\n",
    "nsamples=10 # stochastic x samples used per step\n",
    "nsteps=100 # how many steps we take\n",
    "\n",
    "x_sweep=np.linspace(-4,4,300)\n",
    "\n",
    "for n in range(nsteps):\n",
    "    #############################\n",
    "    # THE CRUCIAL THREE LINES:  #\n",
    "    #############################\n",
    "    x=samples(nsamples) # get random samples\n",
    "    # deviation from true function (vector):\n",
    "    deviation=f(theta,x)-true_f(x)\n",
    "    # do one gradient descent step:\n",
    "    theta-=eta*np.average(deviation[None,:]*f_grad(theta,x),axis=1)\n",
    "    \n",
    "    # Now: Plotting\n",
    "    # compare true function (blue) against\n",
    "    # parametrized function (orange)\n",
    "    # blue dots indicate random points where\n",
    "    # the true function was sampled in this step\n",
    "    clear_output(wait=True)\n",
    "    fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(8,2))\n",
    "    \n",
    "    nlevels=20\n",
    "    ax[0].contourf(X,Y,C,nlevels)\n",
    "    ax[0].contour(X,Y,C,nlevels,colors=\"white\")\n",
    "    ax[0].scatter([theta[0]],[theta[1]],color=\"orange\")\n",
    "    ax[0].set_xlim(theta0s[0],theta0s[-1])\n",
    "    ax[0].set_ylim(theta1s[0],theta1s[-1])\n",
    "    ax[0].set_xlabel(\"theta_0\")\n",
    "    ax[0].set_ylabel(\"theta_1\")    \n",
    "    \n",
    "    ax[1].plot(x_sweep,true_f(x_sweep),color=\"blue\")\n",
    "    ax[1].scatter(x,true_f(x),color=\"blue\")\n",
    "    ax[1].plot(x_sweep,f(theta,x_sweep),color=\"orange\")\n",
    "    ax[1].set_xlim(-4,4)\n",
    "    ax[1].set_ylim(0.0,4.0)\n",
    "    ax[1].set_xlabel(\"x\")\n",
    "    ax[1].set_ylabel(\"f\") \n",
    "    \n",
    "    plt.show()\n",
    "    sleep(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your own examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider for example a parametrized function\n",
    "#       np.sin(theta[1]*(x-theta[0]))/(10.0+x**2)\n",
    "# and a true function (in the shape of a wavepacket)\n",
    "#       np.sin(3.0*(x-1.5))/(10.0+x**2)\n",
    "#\n",
    "# (1) Plot and understand the function\n",
    "# (2) Plot and understand the cost function\n",
    "# (3) Run the fitting (find suitable values for eta etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
