{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Boltzmann Machines applied to MNIST\n",
    "\n",
    "This shows how to train a Boltzmann machine, to sample from an observed probability distribution. It uses the MNIST digits images that are included in every keras installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 9, Homework (this is discussed in session 10)\n",
    "\n",
    "See https://machine-learning-for-physicists.org and the current course website linked there!\n",
    "\n",
    "This notebook is distributed under the Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license:\n",
    "\n",
    "https://creativecommons.org/licenses/by-sa/4.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to:\n",
    "- use a Boltzmann machine to sample from an observed high-dimensional probability distribution (e.g. produce images that look similar to observed training images); applied to the case of MNIST\n",
    "\n",
    "It also implements some of the tricks discussed by the inventor of RBM, G. Hinton, in\n",
    "\n",
    "https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoltzmannStep(v,b,w,do_random_sampling=True):\n",
    "    \"\"\"\n",
    "    Perform a single step of the Markov chain,\n",
    "    going from visible units v to hidden units h,\n",
    "    according to biases b and weights w.\n",
    "    \n",
    "    z_j = b_j + sum_i v_i w_ij\n",
    "    \n",
    "    and P(h_j=1|v) = 1/(exp(-z_j)+1)\n",
    "    \n",
    "    Note: you can go from h to v, by inserting\n",
    "    instead of v the h, instead of b the a, and\n",
    "    instead of w the transpose of w\n",
    "    \"\"\"\n",
    "    batchsize=np.shape(v)[0]\n",
    "    hidden_dim=np.shape(w)[1]\n",
    "    z=b+np.dot(v,w)\n",
    "    P=1/(np.exp(-z)+1)\n",
    "    # now, the usual trick to obtain 0 or 1 according\n",
    "    # to a given probability distribution:\n",
    "    # just produce uniform (in [0,1]) random numbers and\n",
    "    # check whether they are below the cutoff given by P\n",
    "    if do_random_sampling:\n",
    "        p=np.random.uniform(size=[batchsize,hidden_dim])\n",
    "        return(np.array(p<=P,dtype='int'))\n",
    "    else:\n",
    "        return(P) # no binary random output, just the prob. distribution itself!\n",
    "    \n",
    "def BoltzmannSequence(v,a,b,w,drop_h_prime=False,do_random_sampling=True,\n",
    "                      do_not_sample_h_prime=False,\n",
    "                     do_not_sample_v_prime=False):\n",
    "    \"\"\"\n",
    "    Perform one sequence of steps v -> h -> v' -> h'\n",
    "    of a Boltzmann machine, with the given\n",
    "    weights w and biases a and b!\n",
    "    \n",
    "    All the arrays have a shape [batchsize,num_neurons]\n",
    "    (where num_neurons is num_visible for v and\n",
    "    num_hidden for h)\n",
    "    \n",
    "    You can set drop_h_prime to True if you want to\n",
    "    use this routine to generate arbitrarily long sequences\n",
    "    by calling it repeatedly (then don't use h')\n",
    "    Returns: v,h,v',h'\n",
    "    \"\"\"\n",
    "    h=BoltzmannStep(v,b,w,do_random_sampling=do_random_sampling)\n",
    "    if do_not_sample_v_prime:\n",
    "        v_prime=BoltzmannStep(h,a,np.transpose(w),do_random_sampling=False)\n",
    "    else:\n",
    "        v_prime=BoltzmannStep(h,a,np.transpose(w),do_random_sampling=do_random_sampling)\n",
    "    if not drop_h_prime:\n",
    "        if do_not_sample_h_prime: # G. Hinton recommends not sampling in the v'->h' step (reduces noise)\n",
    "            h_prime=BoltzmannStep(v_prime,b,w,do_random_sampling=False)\n",
    "        else:\n",
    "            h_prime=BoltzmannStep(v_prime,b,w,do_random_sampling=do_random_sampling)\n",
    "    else:\n",
    "        h_prime=np.zeros(np.shape(h))\n",
    "    return(v,h,v_prime,h_prime)\n",
    "\n",
    "def trainStep(v,a,b,w,do_random_sampling=True,do_not_sample_h_prime=False,\n",
    "             do_not_sample_v_prime=False):\n",
    "    \"\"\"\n",
    "    Given a set of randomly selected training samples\n",
    "    v (of shape [batchsize,num_neurons_visible]), \n",
    "    and given biases a,b and weights w: update\n",
    "    those biases and weights according to the\n",
    "    contrastive-divergence update rules:\n",
    "    \n",
    "    delta w_ij = eta ( <v_i h_j> - <v'_i h'_j> )\n",
    "    delta a_i  = eta ( <v_i> - <v'_i>)\n",
    "    delta b_j  = eta ( <h_j> - <h'_j>)\n",
    "    \n",
    "    Returns delta_a, delta_b, delta_w, but without the eta factor!\n",
    "    It is up to you to update a,b,w!\n",
    "    \"\"\"\n",
    "    v,h,v_prime,h_prime=BoltzmannSequence(v,a,b,w,do_random_sampling=do_random_sampling,\n",
    "                                         do_not_sample_h_prime=do_not_sample_h_prime,\n",
    "                                         do_not_sample_v_prime=do_not_sample_v_prime)\n",
    "    return( np.average(v,axis=0)-np.average(v_prime,axis=0) ,\n",
    "            np.average(h,axis=0)-np.average(h_prime,axis=0) ,\n",
    "            np.average(v[:,:,None]*h[:,None,:],axis=0)-\n",
    "               np.average(v_prime[:,:,None]*h_prime[:,None,:],axis=0) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_sample_images(batchsize,num_visible,x_train,threshold=0.7,do_digitize=True):\n",
    "    \"\"\"\n",
    "    Produce 'batchsize' samples, of length num_visible.\n",
    "    Returns array v of shape [batchsize,num_visible]\n",
    "    \"\"\"\n",
    "    j=np.random.randint(low=0,high=np.shape(x_train)[0],size=batchsize) # pick random samples\n",
    "    \n",
    "    # reshape suitably, and digitize (so output is 0/1 values)\n",
    "    if do_digitize:\n",
    "        return( np.array( np.reshape(x_train[j,:,:],[batchsize,num_visible])>threshold, dtype='int' ) )\n",
    "    else:\n",
    "        return(  np.reshape(x_train[j,:,:],[batchsize,num_visible]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST data using tensorflow/keras\n",
    "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "x_train=x_train/256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the shape of these arrays\n",
    "print(np.shape(x_train),np.shape(y_train),np.shape(x_test),np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a few images, for fun\n",
    "nimages=10\n",
    "fig,ax=plt.subplots(ncols=nimages,nrows=1,figsize=(nimages,1))\n",
    "for n in range(nimages):\n",
    "    ax[n].imshow(x_train[n,:,:])\n",
    "    ax[n].set_title(str(y_train[n])) # the label\n",
    "    ax[n].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a few random samples:\n",
    "nimages=7\n",
    "Npixels=28\n",
    "samples=produce_sample_images(batchsize=nimages,num_visible=Npixels**2,x_train=x_train)\n",
    "# now unpack them again and display them:\n",
    "fig,ax=plt.subplots(ncols=nimages,nrows=1,figsize=(nimages,1))\n",
    "for n in range(nimages):\n",
    "    ax[n].imshow(np.reshape(samples[n,:],[Npixels,Npixels]))\n",
    "    ax[n].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little trick (useful later): show them all at once, in one imshow\n",
    "# some weird reshape/transpose gymnastics, found by trial and error\n",
    "plt.imshow(np.transpose(np.reshape(np.transpose(np.reshape(samples,[nimages,Npixels,Npixels]),\n",
    "                                                axes=[0,2,1]),[nimages*Npixels,Npixels])))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training, according to the simple principle of an RBM presented in the lecture (all units are binary 0 or 1 all the time, randomly chosen according to the calculated probability distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: the training\n",
    "# here: purely using random binary sampling of all\n",
    "# units at all times (this is not the most efficient way,\n",
    "# but implements directly the basic principle shown in the lecture)\n",
    "\n",
    "Npixels=28\n",
    "num_visible=Npixels**2\n",
    "num_hidden=60\n",
    "batchsize=50\n",
    "eta=0.1\n",
    "nsteps=10000\n",
    "skipsteps=10\n",
    "\n",
    "a=np.random.randn(num_visible)\n",
    "b=np.random.randn(num_hidden)\n",
    "w=0.01*np.random.randn(num_visible,num_hidden)\n",
    "\n",
    "# test_samples=np.zeros([num_visible,nsteps])\n",
    "\n",
    "for j in range(nsteps):\n",
    "    v=produce_sample_images(batchsize,num_visible,x_train)\n",
    "    da,db,dw=trainStep(v,a,b,w)\n",
    "    a+=eta*da\n",
    "    b+=eta*db\n",
    "    w+=eta*dw\n",
    "    print(\"{:05d} / {:05d}\".format(j,nsteps),end=\"   \\r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Now: visualize the typical samples generated (from some starting point)\n",
    "# run several times to continue this. It basically is a random walk\n",
    "# through the space of all possible configurations, hopefully according\n",
    "# to the probability distribution that has been trained!\n",
    "\n",
    "nsteps=1000\n",
    "num_samples=20\n",
    "test_samples=np.zeros([num_samples,num_visible])\n",
    "skipsteps=1\n",
    "substeps=400 # how many steps to take before showing a new picture\n",
    "\n",
    "v_prime=np.zeros(num_visible)\n",
    "h=np.zeros(num_hidden)\n",
    "h_prime=np.zeros(num_hidden)\n",
    "\n",
    "for j in range(nsteps):\n",
    "    for k in range(substeps):\n",
    "        v,h,v_prime,h_prime=BoltzmannSequence(v,a,b,w,drop_h_prime=True) # step from v via h to v_prime!\n",
    "    test_samples[j%num_samples,:]=v[0,:]\n",
    "    v=np.copy(v_prime) # use the new v as a starting point for next step!\n",
    "    if j%skipsteps==0 or j==nsteps-1:\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(np.transpose(np.reshape(np.transpose(np.reshape(test_samples,[num_samples,Npixels,Npixels]),\n",
    "                                                axes=[0,2,1]),[num_samples*Npixels,Npixels])),\n",
    "                  interpolation='none')\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More advanced training: do not randomly sample h' and v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: the training\n",
    "#\n",
    "# Here we use the more sophisticated approach, where\n",
    "# h' and v' are not binary (not randomly sampled), rather\n",
    "# they are taken as the probability distribution (numbers\n",
    "# between 0 and 1). This is a trick recommend by G. Hinton\n",
    "# in his review of Boltzmann Machines. It effectively means\n",
    "# less sampling noise.\n",
    "#\n",
    "# Also, we initialize the biases and weights according to the\n",
    "# tricks given in that review!\n",
    "\n",
    "Npixels=28\n",
    "num_visible=Npixels**2\n",
    "num_hidden=60\n",
    "batchsize=10\n",
    "eta=0.0001\n",
    "nsteps=10*30000\n",
    "skipsteps=10\n",
    "\n",
    "# get average brightness of training images:\n",
    "p_avg=np.average(np.reshape(x_train,[np.shape(x_train)[0],Npixels**2]),axis=0)\n",
    "a=np.log(p_avg/(1.0+1e-6-p_avg)+1e-6) # recipe for visible biases\n",
    "b=np.zeros(num_hidden) # recipe for hidden biases\n",
    "w=0.01*np.random.randn(num_visible,num_hidden) # recipe for weights\n",
    "\n",
    "# test_samples=np.zeros([num_visible,nsteps])\n",
    "\n",
    "for j in range(nsteps):\n",
    "    v=produce_sample_images(batchsize,num_visible,x_train,\n",
    "                           do_digitize=False)\n",
    "    da,db,dw=trainStep(v,a,b,w,\n",
    "                      do_not_sample_h_prime=True,\n",
    "                       do_not_sample_v_prime=True)\n",
    "    a+=eta*da\n",
    "    b+=eta*db\n",
    "    w+=eta*dw\n",
    "    print(\"{:06d} / {:06d}\".format(j,nsteps),end=\"   \\r\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: visualize the typical samples generated (from some starting point)\n",
    "# run several times to continue this. It basically is a random walk\n",
    "# through the space of all possible configurations, hopefully according\n",
    "# to the probability distribution that has been trained!\n",
    "\n",
    "nsteps=20\n",
    "num_samples=20\n",
    "test_samples=np.zeros([num_samples,batchsize,num_visible])\n",
    "test_hidden=np.zeros([num_samples,batchsize,num_hidden])\n",
    "skipsteps=1\n",
    "substeps=400 # how many steps to take before showing a new picture\n",
    "\n",
    "v_prime=np.zeros([batchsize,num_visible])\n",
    "h=np.zeros([batchsize,num_hidden])\n",
    "h_prime=np.zeros([batchsize,num_hidden])\n",
    "\n",
    "v=produce_sample_images(batchsize,num_visible,x_train,\n",
    "                       do_digitize=False)\n",
    "    \n",
    "for j in range(nsteps):\n",
    "    for k in range(substeps):\n",
    "        v,h,v_prime,h_prime=BoltzmannSequence(v,a,b,w,\n",
    "                                  drop_h_prime=True,\n",
    "                                  do_not_sample_v_prime=True) # step from v via h to v_prime!\n",
    "    test_samples[j%num_samples,:,:]=v[:,:]\n",
    "    test_hidden[j%num_samples,:]=h[:,:]\n",
    "    v=np.copy(v_prime) # use the new v as a starting point for next step!\n",
    "    if j%skipsteps==0 or j==nsteps-1:\n",
    "        clear_output(wait=True)\n",
    "        fig,ax=plt.subplots(ncols=1,nrows=batchsize,figsize=(num_samples,batchsize))\n",
    "        for n in range(batchsize):\n",
    "            ax[n].imshow(np.transpose(np.reshape(np.transpose(np.reshape(test_samples[:,n,:],[num_samples,Npixels,Npixels]),\n",
    "                                                    axes=[0,2,1]),[num_samples*Npixels,Npixels])),\n",
    "                      interpolation='none')\n",
    "            ax[n].axis('off')\n",
    "        plt.show()\n",
    "        fig,ax=plt.subplots(ncols=1,nrows=batchsize,figsize=(num_samples,batchsize))\n",
    "        for n in range(batchsize):\n",
    "            ax[n].imshow(np.transpose(np.reshape(np.transpose(np.reshape(test_hidden[:,n,:],[num_samples,6,10]),\n",
    "                                                    axes=[0,2,1]),[num_samples*10,6])),\n",
    "                      interpolation='none')\n",
    "            ax[n].axis('off')\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
