{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a network on representing an image (pure python) and interpreting the role of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 3, Code shown in the lecture\n",
    "\n",
    "See https://machine-learning-for-physicists.org and the current course website linked there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to:\n",
    "- train a neural network trying to approximate an image\n",
    "- 'opening the box': visualize what happens when some neurons are switched off\n",
    "\n",
    "This uses the python backpropagation code shown in lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on programming style: I wanted to keep the code as close as possible to the backpropagation code in the lecture, which uses global variables (e.g. for the weights) for simplicity. However, if you wanted to nicely encapsulate this for usage by other people in larger projects, you would either (1) always make sure to pass along these variables as arguments (and never make them global) or (2) use object-oriented programming and turn this into a network class, which keeps these variables internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, zeros, exp, random, dot, shape, transpose, reshape, meshgrid, linspace, sqrt\n",
    "\n",
    "from scipy import ndimage # for image loading/processing\n",
    "import imageio\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# for updating display \n",
    "# (very simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement backpropagation for a general (fully connected) network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a ReLU unit (rectified linear), which\n",
    "# works better for training in this case\n",
    "# def net_f_df(z): # calculate f(z) and f'(z)\n",
    "#     val=z*(z>0)\n",
    "#     return(val,z>0) # return both f and f'\n",
    "def net_f_df(z):\n",
    "    return([1/(1+exp(-z)), \n",
    "        1/((1+exp(-z))*(1+exp(z))) ]) # sigmoid\n",
    "\n",
    "def forward_step(y,w,b): # calculate values in next layer, from input y\n",
    "    z=dot(y,w)+b # w=weights, b=bias vector for next layer\n",
    "    return(net_f_df(z)) # apply nonlinearity and return result\n",
    "\n",
    "def apply_net(y_in): # one forward pass through the network\n",
    "    global Weights, Biases, NumLayers\n",
    "    global y_layer, df_layer # for storing y-values and df/dz values\n",
    "    \n",
    "    y=y_in # start with input values\n",
    "    y_layer[0]=y\n",
    "    for j in range(NumLayers): # loop through all layers [not counting input]\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j]) # one step, into layer j\n",
    "        df_layer[j]=df # store f'(z) [needed later in backprop]\n",
    "        y_layer[j+1]=y # store f(z) [also needed in backprop]        \n",
    "    return(y)\n",
    "\n",
    "def apply_net_simple(y_in): # one forward pass through the network\n",
    "    # no storage for backprop (this is used for simple tests)\n",
    "\n",
    "    y=y_in # start with input values\n",
    "    y_layer[0]=y\n",
    "    for j in range(NumLayers): # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j]) # one step\n",
    "    return(y)\n",
    "\n",
    "def backward_step(delta,w,df): \n",
    "    # delta at layer N, of batchsize x layersize(N))\n",
    "    # w between N-1 and N [layersize(N-1) x layersize(N) matrix]\n",
    "    # df = df/dz at layer N-1, of batchsize x layersize(N-1)\n",
    "    return( dot(delta,transpose(w))*df )\n",
    "\n",
    "def backprop(y_target): # one backward pass through the network\n",
    "    # the result will be the 'dw_layer' matrices that contain\n",
    "    # the derivatives of the cost function with respect to\n",
    "    # the corresponding weight\n",
    "    global y_layer, df_layer, Weights, Biases, NumLayers\n",
    "    global dw_layer, db_layer # dCost/dw and dCost/db (w,b=weights,biases)\n",
    "    global batchsize\n",
    "    \n",
    "    delta=(y_layer[-1]-y_target)*df_layer[-1]\n",
    "    dw_layer[-1]=dot(transpose(y_layer[-2]),delta)/batchsize\n",
    "    db_layer[-1]=delta.sum(0)/batchsize\n",
    "    for j in range(NumLayers-1):\n",
    "        delta=backward_step(delta,Weights[-1-j],df_layer[-2-j])\n",
    "        dw_layer[-2-j]=dot(transpose(y_layer[-3-j]),delta)/batchsize\n",
    "        db_layer[-2-j]=delta.sum(0)/batchsize\n",
    "        \n",
    "def gradient_step(eta): # update weights & biases (after backprop!)\n",
    "    global dw_layer, db_layer, Weights, Biases\n",
    "    \n",
    "    for j in range(NumLayers):\n",
    "        Weights[j]-=eta*dw_layer[j]\n",
    "        Biases[j]-=eta*db_layer[j]\n",
    "        \n",
    "def train_net(y_in,y_target,eta): # one full training batch\n",
    "    # y_in is an array of size batchsize x (input-layer-size)\n",
    "    # y_target is an array of size batchsize x (output-layer-size)\n",
    "    # eta is the stepsize for the gradient descent\n",
    "    global y_out_result\n",
    "    \n",
    "    y_out_result=apply_net(y_in)\n",
    "    backprop(y_target)\n",
    "    gradient_step(eta)\n",
    "    cost=0.5*((y_target-y_out_result)**2).sum()/batchsize\n",
    "    return(cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train net to reproduce a 2D function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pixel image!\n",
    "face =imageio.imread('Smiley.png')\n",
    "pixel_image=transpose(face[:,:,0]) # have to transpose...\n",
    "pixel_image=pixel_image[:,::-1] # and flip... to get the right view!\n",
    "pixel_image-=pixel_image.min()\n",
    "pixel_image=(pixel_image.astype(dtype='float'))/pixel_image.max() # normalize between 0 and 1!\n",
    "Npixels=shape(pixel_image)[0] # assuming a square image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function we want to have (desired outcome)\n",
    "# this picks the pixels from the image\n",
    "def myFunc(x0,x1):\n",
    "    global pixel_image, Npixels\n",
    "    # convert to integer coordinates (assuming input is 0..1)\n",
    "    x0int=(x0*Npixels*0.9999).astype(dtype='int')\n",
    "    x1int=(x1*Npixels*0.9999).astype(dtype='int')\n",
    "    return(pixel_image[x0int,x1int]) # extract color values at these pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that this works:\n",
    "Npixels_Test=50 # do the test output on a low-res grid! (saves time)\n",
    "xrange=linspace(0,1,Npixels_Test)\n",
    "X0,X1=meshgrid(xrange,xrange)\n",
    "plt.imshow(myFunc(X0,X1),interpolation='nearest',origin='lower')\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 'batchsize' random positions in the 2D square\n",
    "def make_batch():\n",
    "    global batchsize\n",
    "\n",
    "    inputs=random.uniform(low=0,high=1,size=[batchsize,2])\n",
    "    targets=zeros([batchsize,1]) # must have right dimensions\n",
    "    targets[:,0]=myFunc(inputs[:,0],inputs[:,1])\n",
    "    return(inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up all the weights and biases\n",
    "\n",
    "NumLayers=3 # does not count input-layer (but does count output)\n",
    "LayerSizes=[2,150,100,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "Weights=[5*(1./sqrt(LayerSizes[j]))*random.randn(LayerSizes[j],LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "Biases=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# set up all the helper variables\n",
    "\n",
    "y_layer=[zeros(LayerSizes[j]) for j in range(NumLayers+1)]\n",
    "df_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "dw_layer=[zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "db_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# a 'test' batch that includes all the points on the image grid\n",
    "test_batchsize=shape(X0)[0]*shape(X0)[1]\n",
    "testsample=zeros([test_batchsize,2])\n",
    "testsample[:,0]=X0.flatten()\n",
    "testsample[:,1]=X1.flatten()\n",
    "\n",
    "# check the output of this net\n",
    "testoutput=apply_net_simple(testsample)\n",
    "\n",
    "# show this!\n",
    "myim=plt.imshow(reshape(testoutput,shape(X0)),origin='lower',interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the batchsize\n",
    "batchsize=1000\n",
    "\n",
    "# Now: the training! (and plot the cost function)\n",
    "eta=.01\n",
    "batches=2000\n",
    "costs=zeros(batches)\n",
    "\n",
    "for k in range(batches):\n",
    "    y_in,y_target=make_batch()\n",
    "    costs[k]=train_net(y_in,y_target,eta)\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 'test' batch that includes all the points on the image grid\n",
    "test_batchsize=shape(X0)[0]*shape(X0)[1]\n",
    "testsample=zeros([test_batchsize,2])\n",
    "testsample[:,0]=X0.flatten()\n",
    "testsample[:,1]=X1.flatten()\n",
    "\n",
    "# check the output of this net\n",
    "testoutput=apply_net_simple(testsample)\n",
    "\n",
    "# show this!\n",
    "myim=plt.imshow(reshape(testoutput,shape(X0)),origin='lower',interpolation='nearest',vmin=0.0,vmax=1.0)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animate learning progress\n",
    "\n",
    "You need about a million or so samples to get something reasonable looking\n",
    "i.e. at least resembling the shape\n",
    "\n",
    "EXERCISE: find better parameters, for weight initialization, for learning rate, for batchsize etc., to reduce the number of samples needed!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up all the weights and biases\n",
    "\n",
    "NumLayers=3 # does not count input-layer (but does count output)\n",
    "LayerSizes=[2,150,100,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "Weights=[5*(1./sqrt(LayerSizes[j]))*random.randn(LayerSizes[j],LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "Biases=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# set up all the helper variables\n",
    "\n",
    "y_layer=[zeros(LayerSizes[j]) for j in range(NumLayers+1)]\n",
    "df_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "dw_layer=[zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "db_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test sample\n",
    "Npixels_Test=30 # do the test output on a low-res grid! (saves time)\n",
    "xrange=linspace(0,1,Npixels_Test)\n",
    "X0,X1=meshgrid(xrange,xrange)\n",
    "test_batchsize=shape(X0)[0]*shape(X0)[1]\n",
    "testsample=zeros([test_batchsize,2])\n",
    "testsample[:,0]=X0.flatten()\n",
    "testsample[:,1]=X1.flatten()\n",
    "\n",
    "# parameters\n",
    "eta=1.0\n",
    "nsteps=10000\n",
    "nskip_steps=100\n",
    "batchsize=200\n",
    "\n",
    "samples_count=0\n",
    "costs=zeros(nsteps)\n",
    "\n",
    "for j in range(nsteps):\n",
    "    y_in,y_target=make_batch()\n",
    "    costs[j]=train_net(y_in,y_target,eta)\n",
    "    samples_count+=batchsize\n",
    "    testoutput=apply_net_simple(testsample)  \n",
    "    if j%nskip_steps==0: # time to plot again!\n",
    "        clear_output(wait=True)\n",
    "        fig,ax = plt.subplots(ncols=2,nrows=1,figsize=(10,5))\n",
    "        img=ax[1].imshow(reshape(testoutput,shape(X0)),origin='lower',interpolation='nearest',vmin=0)\n",
    "        fig.colorbar(img,ax=ax[1])\n",
    "        ax[1].axis('off') # no axes\n",
    "        ax[0].plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "AllOldWeights=Weights # backup all weights\n",
    "OldWeights=Weights[-1] # especially of last layer, which we will modify below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the resulting network\n",
    "#savez_compressed(\"ImageCompression_Network_Smiley.npz\",LayerSizes,Weights,Biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch on only individual neurons of last hidden layer\n",
    "# and plot the resulting pictures in a big 10x10 array!\n",
    "#data=load(\"ImageCompression_Network_Smiley.npz\")\n",
    "#Weights=data['arr_1']\n",
    "\n",
    "Nrow=10\n",
    "BigImage=zeros([Nrow*Npixels_Test,Nrow*Npixels_Test])\n",
    "for which in range(100):\n",
    "    Weights[-1]=OldWeights.copy()\n",
    "    Weights[-1][0:which-1,:]=0\n",
    "    Weights[-1][which+1:-1,:]=0\n",
    "    testoutput=apply_net_simple(testsample)\n",
    "    row=int(which/Nrow)\n",
    "    column=which%Nrow\n",
    "    BigImage[Npixels_Test*row:Npixels_Test*(row+1),Npixels_Test*column:Npixels_Test*(column+1)]=reshape(testoutput,shape(X0))\n",
    "    #print(row,column)\n",
    "myim=plt.imshow(BigImage,origin='lower',interpolation='nearest',vmin=0.0,vmax=1.0)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
