{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction via t-SNE\n",
    "\n",
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 5, Tutorial (this is discussed in session 5)\n",
    "\n",
    "See https://machine-learning-for-physicists.org and the current course website linked there!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following python code for t-SNE is an original code by the inventor of t-SNE, Laurens van der Maaten. It is available on his website https://lvdmaaten.github.io/tsne/ . \n",
    "\n",
    "It is stated on that website: \"*You are free to use, modify, or redistribute this software in any way you want, but only for non-commercial purposes. The use of the software is at your own risk; the authors are not responsible for any damage as a result from errors in the software.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# for updating display \n",
    "# (very simple animation)\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  tsne.py\n",
    "#\n",
    "# Implementation of t-SNE in Python. The implementation was tested on Python\n",
    "# 2.7.10, and it requires a working installation of NumPy. The implementation\n",
    "# comes with an example on the MNIST dataset. In order to plot the\n",
    "# results of this example, a working installation of matplotlib is required.\n",
    "#\n",
    "# The example can be run by executing: `ipython tsne.py`\n",
    "#\n",
    "#\n",
    "#  Created by Laurens van der Maaten on 20-12-08.\n",
    "#  Copyright (c) 2008 Tilburg University. All rights reserved.\n",
    "\n",
    "# note by FM: For this notebook, the MNIST example was dropped\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def Hbeta(D=np.array([]), beta=1.0):\n",
    "    \"\"\"\n",
    "        Compute the perplexity and the P-row for a specific value of the\n",
    "        precision of a Gaussian distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute P-row and corresponding perplexity\n",
    "    P = np.exp(-D.copy() * beta)\n",
    "    sumP = sum(P)\n",
    "    H = np.log(sumP) + beta * np.sum(D * P) / sumP\n",
    "    P = P / sumP\n",
    "    return H, P\n",
    "\n",
    "\n",
    "def x2p(X=np.array([]), tol=1e-5, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Performs a binary search to get P-values in such a way that each\n",
    "        conditional Gaussian has the same perplexity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize some variables\n",
    "    print(\"Computing pairwise distances...\")\n",
    "    (n, d) = X.shape\n",
    "    sum_X = np.sum(np.square(X), 1)\n",
    "    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
    "    P = np.zeros((n, n))\n",
    "    beta = np.ones((n, 1))\n",
    "    logU = np.log(perplexity)\n",
    "\n",
    "    # Loop over all datapoints\n",
    "    for i in range(n):\n",
    "\n",
    "        # Print progress\n",
    "        if i % 500 == 0:\n",
    "            print(\"Computing P-values for point %d of %d...\" % (i, n))\n",
    "\n",
    "        # Compute the Gaussian kernel and entropy for the current precision\n",
    "        betamin = -np.inf\n",
    "        betamax = np.inf\n",
    "        Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\n",
    "        (H, thisP) = Hbeta(Di, beta[i])\n",
    "\n",
    "        # Evaluate whether the perplexity is within tolerance\n",
    "        Hdiff = H - logU\n",
    "        tries = 0\n",
    "        while np.abs(Hdiff) > tol and tries < 50:\n",
    "\n",
    "            # If not, increase or decrease precision\n",
    "            if Hdiff > 0:\n",
    "                betamin = beta[i].copy()\n",
    "                if betamax == np.inf or betamax == -np.inf:\n",
    "                    beta[i] = beta[i] * 2.\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamax) / 2.\n",
    "            else:\n",
    "                betamax = beta[i].copy()\n",
    "                if betamin == np.inf or betamin == -np.inf:\n",
    "                    beta[i] = beta[i] / 2.\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamin) / 2.\n",
    "\n",
    "            # Recompute the values\n",
    "            (H, thisP) = Hbeta(Di, beta[i])\n",
    "            Hdiff = H - logU\n",
    "            tries += 1\n",
    "\n",
    "        # Set the final row of P\n",
    "        P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP\n",
    "\n",
    "    # Return final P-matrix\n",
    "    print(\"Mean value of sigma: %f\" % np.mean(np.sqrt(1 / beta)))\n",
    "    return P\n",
    "\n",
    "\n",
    "def pca(X=np.array([]), no_dims=50):\n",
    "    \"\"\"\n",
    "        Runs PCA on the NxD array X in order to reduce its dimensionality to\n",
    "        no_dims dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preprocessing the data using PCA...\")\n",
    "    (n, d) = X.shape\n",
    "    X = X - np.tile(np.mean(X, 0), (n, 1))\n",
    "    (l, M) = np.linalg.eig(np.dot(X.T, X))\n",
    "    Y = np.dot(X, M[:, 0:no_dims])\n",
    "    return Y\n",
    "\n",
    "\n",
    "def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0, \n",
    "         do_animation=False, animation_skip_steps=10, max_iter = 1000):\n",
    "    \"\"\"\n",
    "        Runs t-SNE on the dataset in the NxD array X to reduce its\n",
    "        dimensionality to no_dims dimensions. The syntaxis of the function is\n",
    "        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\n",
    "        \n",
    "        Added by F. Marquardt: do_animation==True will give you a graphical animation of\n",
    "        the progress, use animation_skip_steps to control how often this will\n",
    "        be plotted; max_iter controls the total number of gradient descent steps\n",
    "    \"\"\"\n",
    "\n",
    "    # Check inputs\n",
    "    if isinstance(no_dims, float):\n",
    "        print(\"Error: array X should have type float.\")\n",
    "        return -1\n",
    "    if round(no_dims) != no_dims:\n",
    "        print(\"Error: number of dimensions should be an integer.\")\n",
    "        return -1\n",
    "\n",
    "    # Initialize variables\n",
    "    X = pca(X, initial_dims).real\n",
    "    (n, d) = X.shape\n",
    "    initial_momentum = 0.5\n",
    "    final_momentum = 0.8\n",
    "    eta = 500\n",
    "    min_gain = 0.01\n",
    "    Y = np.random.randn(n, no_dims)\n",
    "    dY = np.zeros((n, no_dims))\n",
    "    iY = np.zeros((n, no_dims))\n",
    "    gains = np.ones((n, no_dims))\n",
    "\n",
    "    # Compute P-values\n",
    "    P = x2p(X, 1e-5, perplexity)\n",
    "    P = P + np.transpose(P)\n",
    "    P = P / np.sum(P)\n",
    "    P = P * 4.\t\t\t\t\t\t\t\t\t# early exaggeration\n",
    "    P = np.maximum(P, 1e-12)\n",
    "\n",
    "    if do_animation: # added by FM\n",
    "        costs=np.zeros(max_iter) # to store the cost values\n",
    "        \n",
    "    # Run iterations\n",
    "    for iter in range(max_iter):\n",
    "\n",
    "        # Compute pairwise affinities\n",
    "        sum_Y = np.sum(np.square(Y), 1)\n",
    "        num = -2. * np.dot(Y, Y.T)\n",
    "        num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))\n",
    "        num[range(n), range(n)] = 0.\n",
    "        Q = num / np.sum(num)\n",
    "        Q = np.maximum(Q, 1e-12)\n",
    "\n",
    "        # Compute gradient\n",
    "        PQ = P - Q\n",
    "        for i in range(n):\n",
    "            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)\n",
    "\n",
    "        # Perform the update\n",
    "        if iter < 20:\n",
    "            momentum = initial_momentum\n",
    "        else:\n",
    "            momentum = final_momentum\n",
    "        gains = (gains + 0.2) * ((dY > 0.) != (iY > 0.)) + \\\n",
    "                (gains * 0.8) * ((dY > 0.) == (iY > 0.))\n",
    "        gains[gains < min_gain] = min_gain\n",
    "        iY = momentum * iY - eta * (gains * dY)\n",
    "        Y = Y + iY\n",
    "        Y = Y - np.tile(np.mean(Y, 0), (n, 1))\n",
    "\n",
    "        if not do_animation: # added by FM: do not print if we are animating!\n",
    "            # Compute current value of cost function\n",
    "            if (iter + 1) % 10 == 0:\n",
    "                C = np.sum(P * np.log(P / Q))\n",
    "                print(\"Iteration %d: error is %f\" % (iter + 1, C), end=\"           \\r\") # modified to overwrite line\n",
    "\n",
    "        # Stop lying about P-values\n",
    "        if iter == 100:\n",
    "            P = P / 4.\n",
    "            \n",
    "        if do_animation:  # added by FM\n",
    "            C = np.sum(P * np.log(P / Q)) # compute for every step, to store it in 'costs'\n",
    "            costs[iter]=C\n",
    "            if iter % animation_skip_steps==0:\n",
    "                clear_output(wait=True)\n",
    "                fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(10,5))\n",
    "                ax[0].plot(costs)\n",
    "                ax[1].scatter(Y[:,0],Y[:,1],color=\"orange\")\n",
    "                plt.show()\n",
    "\n",
    "    # Return solution\n",
    "    return Y\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\")\n",
    "#     print(\"Running example on 2,500 MNIST digits...\")\n",
    "#     X = np.loadtxt(\"mnist2500_X.txt\")\n",
    "#     labels = np.loadtxt(\"mnist2500_labels.txt\")\n",
    "#     Y = tsne(X, 2, 50, 20.0)\n",
    "#     pylab.scatter(Y[:, 0], Y[:, 1], 20, labels)\n",
    "#     pylab.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: A few Gaussian clouds in high-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a high-dimensional data set, composed of a few Gaussian point clouds in high-dimensional space\n",
    "\n",
    "n_dim=100 # a really high-dimensional space\n",
    "\n",
    "n_clusters=5 # number of clusters, i.e. clouds\n",
    "N_cluster_points=30 # number of points inside each cluster\n",
    "N=n_clusters*N_cluster_points # total number of points\n",
    "\n",
    "Gauss_spread=1.0 # size of each cluster (cloud)\n",
    "\n",
    "X=np.zeros([N,n_dim])\n",
    "\n",
    "for j in range(n_clusters):\n",
    "    Xmean=np.random.randn(n_dim) # the center position of the cluster\n",
    "    X[j*N_cluster_points:(j+1)*N_cluster_points,:]=Xmean[None,:]+Gauss_spread*np.random.randn(N_cluster_points,n_dim)\n",
    "\n",
    "# plot these points in a projection into 2D space\n",
    "# color them according to the cluster they belong to!\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "for j in range(n_clusters):\n",
    "    X0=X[j*N_cluster_points:(j+1)*N_cluster_points,0]\n",
    "    X1=X[j*N_cluster_points:(j+1)*N_cluster_points,1]    \n",
    "    plt.scatter(X0,X1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now apply t-SNE to reduce to two dimensions in a smart way!\n",
    "\n",
    "Y=tsne(X, no_dims=2, initial_dims=50, perplexity=20.0, \n",
    "       do_animation=True, animation_skip_steps=10, max_iter=300)\n",
    "\n",
    "# plot the points according to the t-SNE projection into 2D space\n",
    "# color them according to the cluster they belong to!\n",
    "plt.figure(figsize=(8,8))\n",
    "for j in range(n_clusters):\n",
    "    Y0=Y[j*N_cluster_points:(j+1)*N_cluster_points,0]\n",
    "    Y1=Y[j*N_cluster_points:(j+1)*N_cluster_points,1]    \n",
    "    plt.scatter(Y0,Y1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Multiple Gaussians: finding the true number without labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a tSNE-scatterplot, but with some randomly marked points,\n",
    "# together with the corresponding high-dimensional data points plotted\n",
    "# as curves!\n",
    "\n",
    "def plot_tsne_with_curves(y0,y1,x,xlabel,n_picks=10,vmin=-0.1,vmax=2):\n",
    "    random_picks=np.random.randint(np.shape(y0)[0],size=n_picks) # pick some random points\n",
    "    \n",
    "    fig=plt.figure(constrained_layout=True,figsize=(8,4))\n",
    "    gs=fig.add_gridspec(ncols=8,nrows=4)\n",
    "    scatter_plot=fig.add_subplot(gs[0:4,0:4])\n",
    "    \n",
    "    myplot={}\n",
    "    j=0\n",
    "    for n0 in range(4):\n",
    "        for n1 in range(4):\n",
    "            myplot[j]=fig.add_subplot(gs[n0,4+n1])\n",
    "            myplot[j].axis('off')\n",
    "            j+=1\n",
    "    \n",
    "    scatter_plot.scatter(y0,y1,c=xlabel)\n",
    "    scatter_plot.scatter(y0[random_picks],y1[random_picks],color=\"black\",alpha=0.7,s=80)\n",
    "    \n",
    "    for idx in range(len(random_picks)):\n",
    "        scatter_plot.text(y0[random_picks[idx]], y1[random_picks[idx]], \n",
    "                      str(idx), fontsize=8, color=\"orange\", \n",
    "                     alpha=0.8, horizontalalignment='center',\n",
    "                verticalalignment='center')\n",
    "    \n",
    "    for idx,m in enumerate(random_picks):\n",
    "        if idx<j:\n",
    "            myplot[idx].plot(x[m,:])\n",
    "            myplot[idx].text(0.1, 0.75, str(idx), fontsize=12, color=\"orange\", \n",
    "                             alpha=0.5, horizontalalignment='center',\n",
    "                        verticalalignment='center', transform=myplot[idx].transAxes)\n",
    "            myplot[idx].set_ylim([vmin,vmax])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator1D(batchsize,x): # produce a batch of curves, a random number of Gaussian\n",
    "    maxNum=2 # the maximum number of Gaussians\n",
    "    NumGaussians=np.random.randint(maxNum+1,size=batchsize) # select the number, for each sample\n",
    "    Curves=np.zeros([batchsize,len(x)])\n",
    "    for j in range(maxNum):\n",
    "        R=np.random.uniform(low=0.1,high=0.11,size=batchsize) # width\n",
    "        A=np.random.uniform(low=0.9,high=1.0,size=batchsize) # amplitude\n",
    "        x0=np.random.uniform(size=batchsize,low=-0.8,high=0.8) # position\n",
    "        Curves+=(j<=NumGaussians[:,None]-1)*A[:,None]*np.exp(-((x[None,:]-x0[:,None])/R[:,None])**2)\n",
    "    Curves+=0.1*np.random.randn(batchsize,len(x)) # add a bit of background noise on top\n",
    "    return( Curves, NumGaussians )\n",
    "\n",
    "n_dim=100\n",
    "x=np.linspace(-1,1,n_dim)\n",
    "N=2000 # how many curves\n",
    "\n",
    "X,Xlabel=my_generator1D(N,x) \n",
    "# small x is the coordinate, capital X are the high-dim. \"data points\", meaning samples of curves!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=10,nrows=1,figsize=(10,1))\n",
    "for n in range(10):\n",
    "    ax[n].plot(X[n,:])\n",
    "    ax[n].set_ylim([-0.1,2])\n",
    "    ax[n].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot these points in a projection into 2D space\n",
    "\n",
    "# pick two arbitrary coordinates\n",
    "j0=17\n",
    "j1=35\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "X0=X[:,j0]\n",
    "X1=X[:,j1]    \n",
    "plt.scatter(X0,X1,c=Xlabel)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now apply t-SNE to reduce to two dimensions in a smart way!\n",
    "\n",
    "Y=tsne(X, 2, 20, 30.0, do_animation=True, animation_skip_steps=10, max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_curves(Y[:,0],Y[:,1],X,Xlabel,n_picks=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Exercise 1: Change the perplexity parameter in the 'Gaussian cloud' example and/or the 'multiple Gaussians' example and observe its effect!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Exercise 2: Modify the 'multiple Gaussians' example by inventing different sample curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
