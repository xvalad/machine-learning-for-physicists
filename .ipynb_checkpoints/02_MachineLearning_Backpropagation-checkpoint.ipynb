{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Backpropagation with Pure Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 2\n",
    "\n",
    "See https://machine-learning-for-physicists.org and the current course website linked there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to:\n",
    "- implement backpropagation in pure python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically the code shown in the lecture, with a tiny bit of clean-up and extra comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the \"numpy\" library for linear algebra\n",
    "\n",
    "# In the lecture videos, I do this:\n",
    "#\n",
    "# from numpy import *\n",
    "#\n",
    "# WARNING: It is generally considered bad\n",
    "# programming style to \"import *\", as it\n",
    "# can lead to confusion. For me, I\n",
    "#  (1) ALWAYS import numpy\n",
    "#  (2) NEVER import any other package in this * way\n",
    "# Therefore, there is never confusion for me, and\n",
    "# it makes my code a bit more readable (for me).\n",
    "# However, since 99% of people are using the \n",
    "# syntax \"import numpy as np\" and then\n",
    "# access \"np.exp()\" etc., you\n",
    "# should probably also use \"np\" once you start\n",
    "# exchanging code with others. I convert\n",
    "# back to the np. syntax when I turn my\n",
    "# converged code into a module.\n",
    "#\n",
    "# It is apparently officially accepted to explicitly\n",
    "# list all the functions you need from numpy:\n",
    "\n",
    "from numpy import array, zeros, exp, random, dot, shape, reshape, meshgrid, linspace, transpose\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement backpropagation for a general (fully connected) network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_f_df(z): # calculate f(z) and f'(z)\n",
    "    val=1/(1+exp(-z)) # sigmoid\n",
    "    return(val,exp(-z)*(val**2)) # return both f and f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(y,w,b): # calculate values in next layer, from input y\n",
    "    z=dot(y,w)+b # w=weights, b=bias vector for next layer\n",
    "    return(net_f_df(z)) # apply nonlinearity and return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_net(y_in): # one forward pass through the network\n",
    "    global Weights, Biases, NumLayers\n",
    "    global y_layer, df_layer # for storing y-values and df/dz values\n",
    "    \n",
    "    y=y_in # start with input values\n",
    "    y_layer[0]=y\n",
    "    for j in range(NumLayers): # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j]) # one step\n",
    "        df_layer[j]=df # store f'(z) [needed later in backprop]\n",
    "        y_layer[j+1]=y # store f(z) [also needed in backprop]        \n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_net_simple(y_in): # one forward pass through the network\n",
    "    # no storage for backprop (this is used for simple tests)\n",
    "\n",
    "    y=y_in # start with input values\n",
    "    y_layer[0]=y\n",
    "    for j in range(NumLayers): # loop through all layers\n",
    "        # j=0 corresponds to the first layer above the input\n",
    "        y,df=forward_step(y,Weights[j],Biases[j]) # one step\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_step(delta,w,df): \n",
    "    # delta at layer N, of batchsize x layersize(N))\n",
    "    # w between N-1 and N [layersize(N-1) x layersize(N) matrix]\n",
    "    # df = df/dz at layer N-1, of batchsize x layersize(N-1)\n",
    "    return( dot(delta,transpose(w))*df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(y_target): # one backward pass through the network\n",
    "    # the result will be the 'dw_layer' matrices that contain\n",
    "    # the derivatives of the cost function with respect to\n",
    "    # the corresponding weight\n",
    "    global y_layer, df_layer, Weights, Biases, NumLayers\n",
    "    global dw_layer, db_layer # dCost/dw and dCost/db (w,b=weights,biases)\n",
    "    global batchsize\n",
    "    \n",
    "    delta=(y_layer[-1]-y_target)*df_layer[-1]\n",
    "    dw_layer[-1]=dot(transpose(y_layer[-2]),delta)/batchsize\n",
    "    db_layer[-1]=delta.sum(0)/batchsize\n",
    "    for j in range(NumLayers-1):\n",
    "        delta=backward_step(delta,Weights[-1-j],df_layer[-2-j])\n",
    "        dw_layer[-2-j]=dot(transpose(y_layer[-3-j]),delta)\n",
    "        db_layer[-2-j]=delta.sum(0)/batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(eta): # update weights & biases (after backprop!)\n",
    "    global dw_layer, db_layer, Weights, Biases\n",
    "    \n",
    "    for j in range(NumLayers):\n",
    "        Weights[j]-=eta*dw_layer[j]\n",
    "        Biases[j]-=eta*db_layer[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(y_in,y_target,eta): # one full training batch\n",
    "    # y_in is an array of size batchsize x (input-layer-size)\n",
    "    # y_target is an array of size batchsize x (output-layer-size)\n",
    "    # eta is the stepsize for the gradient descent\n",
    "    global y_out_result\n",
    "    \n",
    "    y_out_result=apply_net(y_in)\n",
    "    backprop(y_target)\n",
    "    gradient_step(eta)\n",
    "    cost=((y_target-y_out_result)**2).sum()/batchsize\n",
    "    return(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for a particular set of layer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up all the weights and biases\n",
    "\n",
    "NumLayers=3 # does not count input-layer (but does count output)\n",
    "LayerSizes=[2,20,30,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "# initialize random weights and biases for all layers (except input of course)\n",
    "Weights=[random.uniform(low=-1,high=+1,size=[ LayerSizes[j],LayerSizes[j+1] ]) for j in range(NumLayers)]\n",
    "Biases=[random.uniform(low=-1,high=+1,size=LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# define the batchsize\n",
    "batchsize=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up all the helper variables\n",
    "\n",
    "y_layer=[zeros([batchsize,LayerSizes[j]]) for j in range(NumLayers+1)]\n",
    "df_layer=[zeros([batchsize,LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "dw_layer=[zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "db_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net on one single batch repeatedly (not so good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in=random.uniform(low=-1,high=+1,size=[batchsize,LayerSizes[0]])\n",
    "y_target=random.uniform(low=-1,high=+1,size=[batchsize,LayerSizes[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one training step:\n",
    "train_net(y_in,y_target,.0001) # returns cost function value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta=0.001\n",
    "batches=200\n",
    "\n",
    "costs=zeros(batches) # array to store the costs\n",
    "\n",
    "for k in range(batches):\n",
    "    costs[k]=train_net(y_in,y_target,eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will show a very simple decrease, because\n",
    "# we are not yet stochastically sampling inputs\n",
    "# (it is always the SAME input! so the network\n",
    "# only becomes good for that input)\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce random batches: randomly sample a function defined on a 2D square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a change: Set up rectified linear units (relu) \n",
    "# instead of sigmoid\n",
    "def net_f_df(z): # calculate f(z) and f'(z)\n",
    "    val=z*(z>0)\n",
    "    return(val,z>0) # return both f and f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up all the weights and biases\n",
    "\n",
    "NumLayers=2 # does not count input-layer (but does count output)\n",
    "LayerSizes=[2,100,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "Weights=[random.uniform(low=-0.1,high=+0.1,size=[ LayerSizes[j],LayerSizes[j+1] ]) for j in range(NumLayers)]\n",
    "Biases=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# set up all the helper variables\n",
    "\n",
    "y_layer=[zeros(LayerSizes[j]) for j in range(NumLayers+1)]\n",
    "df_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "dw_layer=[zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "db_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the batchsize\n",
    "batchsize=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc(x0,x1):\n",
    "    r2=x0**2+x1**2\n",
    "    return(exp(-5*r2)*abs(x1+x0))\n",
    "\n",
    "xrange=linspace(-0.5,0.5,40)\n",
    "X0,X1=meshgrid(xrange,xrange)\n",
    "plt.imshow(myFunc(X0,X1),interpolation='nearest',origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    global batchsize\n",
    "\n",
    "    inputs=random.uniform(low=-0.5,high=+0.5,size=[batchsize,2])\n",
    "    targets=zeros([batchsize,1]) # must have right dimensions\n",
    "    targets[:,0]=myFunc(inputs[:,0],inputs[:,1])\n",
    "    return(inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to evaluate the (randomly initialized) network\n",
    "# on some area in the 2D plane\n",
    "test_batchsize=shape(X0)[0]*shape(X0)[1]\n",
    "testsample=zeros([test_batchsize,2])\n",
    "testsample[:,0]=X0.flatten()\n",
    "testsample[:,1]=X1.flatten()\n",
    "\n",
    "testoutput=apply_net_simple(testsample)\n",
    "myim=plt.imshow(reshape(testoutput,shape(X0)),origin='lower',interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now train on randomly sampled points\n",
    "# to make the network reproduce better and\n",
    "# better this 2D function!\n",
    "eta=.001\n",
    "batches=2000\n",
    "costs=zeros(batches)\n",
    "\n",
    "for k in range(batches):\n",
    "    y_in,y_target=make_batch()\n",
    "    costs[k]=train_net(y_in,y_target,eta)\n",
    "\n",
    "plt.plot(costs)\n",
    "plt.title(\"Cost function during training\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animate the network results during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start fresh:\n",
    "\n",
    "# set up all the weights and biases\n",
    "\n",
    "NumLayers=2 # does not count input-layer (but does count output)\n",
    "LayerSizes=[2,100,1] # input-layer,hidden-1,hidden-2,...,output-layer\n",
    "\n",
    "Weights=[random.uniform(low=-0.1,high=+0.1,size=[ LayerSizes[j],LayerSizes[j+1] ]) for j in range(NumLayers)]\n",
    "Biases=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "\n",
    "# set up all the helper variables\n",
    "\n",
    "y_layer=[zeros(LayerSizes[j]) for j in range(NumLayers+1)]\n",
    "df_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]\n",
    "dw_layer=[zeros([LayerSizes[j],LayerSizes[j+1]]) for j in range(NumLayers)]\n",
    "db_layer=[zeros(LayerSizes[j+1]) for j in range(NumLayers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions for updating display \n",
    "# (simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "eta=0.01 # learning rate\n",
    "nsteps=100\n",
    "\n",
    "costs=zeros(nsteps)\n",
    "for j in range(nsteps):\n",
    "    clear_output(wait=True)\n",
    "    fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(8,4)) # prepare figure\n",
    "    ax[1].axis('off') # no axes\n",
    "    \n",
    "    # the crucial lines:\n",
    "    y_in,y_target=make_batch() # random samples (points in 2D)\n",
    "    costs[j]=train_net(y_in,y_target,eta) # train network (one step, on this batch)\n",
    "    testoutput=apply_net_simple(testsample) # check the new network output in the plane\n",
    "    \n",
    "    img=ax[1].imshow(reshape(testoutput,shape(X0)),interpolation='nearest',origin='lower') # plot image\n",
    "    ax[0].plot(costs)\n",
    "    \n",
    "    ax[0].set_title(\"Cost during training\")\n",
    "    ax[0].set_xlabel(\"number of batches\")\n",
    "    ax[1].set_title(\"Current network prediction\")\n",
    "    plt.show()\n",
    "    sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
