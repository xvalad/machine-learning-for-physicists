{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Neural Networks with Pure Python / Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 1, Tutorials\n",
    "\n",
    "See https://machine-learning-for-physicists.org and the current course website linked there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to:\n",
    "- visualize neural networks\n",
    "\n",
    "The networks have 2 input and 1 output neurons, but arbitrarily many hidden layers, and also you can choose the activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy import array, zeros, exp, random, dot, shape, reshape, meshgrid, linspace\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# for nice inset colorbars:\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_layer(y_in,w,b,activation):\n",
    "    \"\"\"\n",
    "    Go from one layer to the next, given a \n",
    "    weight matrix w (shape [n_neurons_in,n_neurons_out])\n",
    "    a bias vector b (length n_neurons_out)\n",
    "    and the values of input neurons y_in \n",
    "    (shape [batchsize,n_neurons_in])\n",
    "    \n",
    "    returns the values of the output neurons in the next layer \n",
    "    (shape [batchsize, n_neurons_out])\n",
    "    \"\"\"\n",
    "    # to understand the following line, watch the beginning of lecture 2\n",
    "    z=np.dot(y_in,w)+b # batch processing: y_in is of shape [batchsize,num_neurons_in]\n",
    "    if activation=='sigmoid':\n",
    "        return(1/(1+np.exp(-z)))\n",
    "    elif activation=='jump':\n",
    "        return(np.array(z>0,dtype='float'))\n",
    "    elif activation=='linear':\n",
    "        return(z)\n",
    "    elif activation=='reLU':\n",
    "        return((z>0)*z)\n",
    "\n",
    "def apply_net(y_in,weights,biases,activations):\n",
    "    \"\"\"\n",
    "    Apply a whole network of multiple layers\n",
    "    \"\"\"\n",
    "    y=y_in\n",
    "    for j in range(len(biases)):\n",
    "        y=apply_layer(y,weights[j],biases[j],activations[j])\n",
    "    return(y)\n",
    "\n",
    "# some internal routines for plotting the network:\n",
    "def plot_connection_line(ax,X,Y,W,vmax=1.0,linewidth=3):\n",
    "    t=np.linspace(0,1,20)\n",
    "    if W>0:\n",
    "        col=[0,0.4,0.8]\n",
    "    else:\n",
    "        col=[1,0.3,0]\n",
    "    ax.plot(X[0]+(3*t**2-2*t**3)*(X[1]-X[0]),Y[0]+t*(Y[1]-Y[0]),\n",
    "           alpha=abs(W)/vmax,color=col,\n",
    "           linewidth=linewidth)\n",
    "    \n",
    "def plot_neuron_alpha(ax,X,Y,B,size=100.0,vmax=1.0):\n",
    "    if B>0:\n",
    "        col=[0,0.4,0.8]\n",
    "    else:\n",
    "        col=[1,0.3,0]\n",
    "    ax.scatter([X],[Y],marker='o',c=col,alpha=abs(B)/vmax,s=size,zorder=10)\n",
    "\n",
    "def plot_neuron(ax,X,Y,B,size=100.0,vmax=1.0):\n",
    "    if B>0:\n",
    "        col=[0,0.4,0.8]\n",
    "    else:\n",
    "        col=[1,0.3,0]\n",
    "    ax.scatter([X],[Y],marker='o',c=col,s=size,zorder=10)\n",
    "    \n",
    "def visualize_network(weights,biases,activations,\n",
    "                      M=100,y0range=[-1,1],y1range=[-1,1],\n",
    "                     size=400.0, linewidth=5.0):\n",
    "    \"\"\"\n",
    "    Visualize a neural network with 2 input \n",
    "    neurons and 1 output neuron (plot output vs input in a 2D plot)\n",
    "    \n",
    "    weights is a list of the weight matrices for the\n",
    "    layers, where weights[j] is the matrix for the connections\n",
    "    from layer j to layer j+1 (where j==0 is the input)\n",
    "    \n",
    "    weights[j][m,k] is the weight for input neuron k going to output neuron m\n",
    "    (note: internally, m and k are swapped, see the explanation of\n",
    "    batch processing in lecture 2)\n",
    "    \n",
    "    biases[j] is the vector of bias values for obtaining the neurons in layer j+1\n",
    "    biases[j][k] is the bias for neuron k in layer j+1\n",
    "    \n",
    "    activations is a list of the activation functions for\n",
    "    the different layers: choose 'linear','sigmoid',\n",
    "    'jump' (i.e. step-function), and 'reLU'\n",
    "    \n",
    "    M is the resolution (MxM grid)\n",
    "    \n",
    "    y0range is the range of y0 neuron values (horizontal axis)\n",
    "    y1range is the range of y1 neuron values (vertical axis)\n",
    "    \"\"\"\n",
    "    swapped_weights=[]\n",
    "    for j in range(len(weights)):\n",
    "        swapped_weights.append(np.transpose(weights[j]))\n",
    "        \n",
    "    y0,y1=np.meshgrid(np.linspace(y0range[0],y0range[1],M),np.linspace(y1range[0],y1range[1],M))\n",
    "    y_in=np.zeros([M*M,2])\n",
    "    y_in[:,0]=y0.flatten()\n",
    "    y_in[:,1]=y1.flatten()\n",
    "    y_out=apply_net(y_in,swapped_weights,biases,activations)\n",
    "\n",
    "    fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(8,4))\n",
    "    \n",
    "    # plot the network itself:\n",
    "    \n",
    "    # positions of neurons on plot:\n",
    "    posX=[[-0.5,+0.5]]; posY=[[0,0]]\n",
    "    vmax=0.0 # for finding the maximum weight\n",
    "    vmaxB=0.0 # for maximum bias\n",
    "    for j in range(len(biases)):\n",
    "        n_neurons=len(biases[j])\n",
    "        posX.append(np.array(range(n_neurons))-0.5*(n_neurons-1))\n",
    "        posY.append(np.full(n_neurons,j+1))\n",
    "        vmax=np.maximum(vmax,np.max(np.abs(weights[j])))\n",
    "        vmaxB=np.maximum(vmaxB,np.max(np.abs(biases[j])))\n",
    "\n",
    "    # plot connections\n",
    "    for j in range(len(biases)):\n",
    "        for k in range(len(posX[j])):\n",
    "            for m in range(len(posX[j+1])):\n",
    "                plot_connection_line(ax[0],[posX[j][k],posX[j+1][m]],\n",
    "                                     [posY[j][k],posY[j+1][m]],\n",
    "                                     swapped_weights[j][k,m],vmax=vmax,\n",
    "                                    linewidth=linewidth)\n",
    "    \n",
    "    # plot neurons\n",
    "    for k in range(len(posX[0])): # input neurons (have no bias!)\n",
    "        plot_neuron(ax[0],posX[0][k],posY[0][k],\n",
    "                   vmaxB,vmax=vmaxB,size=size)\n",
    "    for j in range(len(biases)): # all other neurons\n",
    "        for k in range(len(posX[j+1])):\n",
    "            plot_neuron(ax[0],posX[j+1][k],posY[j+1][k],\n",
    "                       biases[j][k],vmax=vmaxB,size=size)\n",
    "    \n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # now: the output of the network\n",
    "    img=ax[1].imshow(np.reshape(y_out,[M,M]),origin='lower',\n",
    "                    extent=[y0range[0],y0range[1],y1range[0],y1range[1]])\n",
    "    ax[1].set_xlabel(r'$y_0$')\n",
    "    ax[1].set_ylabel(r'$y_1$')\n",
    "    \n",
    "    axins1 = inset_axes(ax[1],\n",
    "                    width=\"40%\",  # width = 50% of parent_bbox width\n",
    "                    height=\"5%\",  # height : 5%\n",
    "                    loc='upper right')\n",
    "\n",
    "    imgmin=np.min(y_out)\n",
    "    imgmax=np.max(y_out)\n",
    "    color_bar=fig.colorbar(img, cax=axins1, orientation=\"horizontal\",ticks=np.linspace(imgmin,imgmax,3))\n",
    "    cbxtick_obj = plt.getp(color_bar.ax.axes, 'xticklabels')\n",
    "    plt.setp(cbxtick_obj, color=\"white\")\n",
    "    axins1.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_network(weights=[ [ \n",
    "    [0.2,0.9]  # weights of 2 input neurons for single output neuron\n",
    "    ] ],\n",
    "    biases=[ \n",
    "        [0.5] # bias for single output neuron\n",
    "            ],\n",
    "    activations=[ 'sigmoid' # activation for output\n",
    "                ],\n",
    "    y0range=[-3,3],y1range=[-3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_network(weights=[ [ \n",
    "    [0.2,0.9],  # weights of 2 input neurons for 1st hidden neuron\n",
    "    [-0.5,0.3], # weights of 2 input neurons for 2nd hidden\n",
    "    [0.8,-1.3]  # weights of 2 input neurons for 3rd hidden\n",
    "    ],                 \n",
    "    [ \n",
    "        [-0.3,0.7,0.5] # weights of 3 hidden neurons for output\n",
    "    ]  ],\n",
    "    biases=[ \n",
    "        [0.1,-0.5,-0.5], # biases of 3 hidden neurons\n",
    "        [-.2] # bias for output neuron\n",
    "            ],\n",
    "    activations=[ 'jump', # activation for hidden\n",
    "                'jump' # activation for output\n",
    "                ],\n",
    "    y0range=[-3,3],y1range=[-3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_network(weights=[ [ \n",
    "    [0.2,0.9],  # weights of 2 input neurons for 1st hidden neuron\n",
    "    [-0.5,0.3], # weights of 2 input neurons for 2nd hidden\n",
    "    [0.8,-1.3]  # weights of 2 input neurons for 3rd hidden\n",
    "    ],                 \n",
    "    [ \n",
    "        [-0.3,0.7,0.5] # weights of 3 hidden neurons for output\n",
    "    ]  ],\n",
    "    biases=[ \n",
    "        [0.1,-0.5,-0.5], # biases of 3 hidden neurons\n",
    "        [-.2] # bias for output neuron\n",
    "            ],\n",
    "    activations=[ 'jump', # activation for hidden\n",
    "                'linear' # activation for output\n",
    "                ],\n",
    "    y0range=[-3,3],y1range=[-3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same, but with sigmoid (and more compactly written)\n",
    "visualize_network(weights=[ \n",
    "    [ [0.2,0.9],  [-0.5,0.3], [0.8,-1.3]  ],                 \n",
    "    [ [-0.3,0.7,0.5] ]  \n",
    "    ],\n",
    "    biases=[ \n",
    "        [0.1,-0.5,-0.5],\n",
    "        [0.5]\n",
    "    ],\n",
    "    activations=[ 'sigmoid', # activation for hidden\n",
    "                'linear' # activation for output\n",
    "                ],\n",
    "    y0range=[-3,3],y1range=[-3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sharper sigmoid: scale all weights and biases!\n",
    "factor=10.0\n",
    "# this needs np.array(), because you cannot do factor*<python-list>\n",
    "\n",
    "visualize_network(weights=[ \n",
    "    factor*np.array([ [0.2,0.9],  [-0.5,0.3], [0.8,-1.3]  ]),                 \n",
    "    factor*np.array([ [-0.3,0.7,0.5] ])\n",
    "    ],\n",
    "    biases=[ \n",
    "        factor*np.array([0.1,-0.5,-0.5]),\n",
    "        factor*np.array([0.5])\n",
    "    ],\n",
    "    activations=[ 'sigmoid', # activation for hidden\n",
    "                'linear' # activation for output\n",
    "                ],\n",
    "    y0range=[-3,3],y1range=[-3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with 5 intermediate neurons, for fun:\n",
    "\n",
    "visualize_network(weights=[ [ [0.2,0.9],[-0.5,0.3],[0.8,-1.3],\n",
    "                            [-0.3,-0.9], [-0.8,-1.2] ], \n",
    "                           [ [-0.3,0.7,0.5,-0.3,0.4] ]  ],\n",
    "                 biases=[ [0.1,-0.5,-0.5,0.3,0.2], [0.5] ],\n",
    "                 activations=[ 'jump', 'linear' ],\n",
    "                 y0range=[-3,3],y1range=[-3,3], M=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many superimposed lines\n",
    "# this can be used to construct arbitrary shapes\n",
    "# with only a single hidden layer\n",
    "\n",
    "n_lines=10\n",
    "phi=np.linspace(0,2*np.pi,n_lines+1)[0:n_lines]\n",
    "\n",
    "weight_array=np.zeros([n_lines,2])\n",
    "weight_array[:,0]=factor*np.cos(phi)\n",
    "weight_array[:,1]=factor*np.sin(phi)\n",
    "bias_array=np.full(n_lines,factor*(+0.5))\n",
    "\n",
    "# sharper sigmoid: scale all weights and biases!\n",
    "factor=20.0\n",
    "# this needs np.array(), because you cannot do factor*<python-list>\n",
    "visualize_network(weights=[ \n",
    "    weight_array, \n",
    "    np.full([1,n_lines],1.0)\n",
    "    ],\n",
    "    biases=[ \n",
    "        bias_array,\n",
    "        [0.0]\n",
    "    ],\n",
    "    activations=[ 'sigmoid', # activation for hidden\n",
    "                'linear' # activation for output\n",
    "                ],\n",
    "    y0range=[-3,3],y1range=[-3,3],\n",
    "                 size=30.0,M=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two blobs, each constructed using many superimposed lines\n",
    "# this can be used to construct arbitrary shapes\n",
    "# with only a single hidden layer\n",
    "\n",
    "n_lines=30\n",
    "phi=np.linspace(0,2*np.pi,n_lines+1)[0:n_lines]\n",
    "\n",
    "shifts=[-0.3,1.5]\n",
    "n_blobs=len(shifts)\n",
    "full_weight_array=np.zeros([n_blobs*n_lines,2])\n",
    "full_bias_array=np.zeros(n_blobs*n_lines)\n",
    "\n",
    "j=0\n",
    "for shift in shifts:\n",
    "    weight_array=np.zeros([n_lines,2])\n",
    "    weight_array[:,0]=factor*np.cos(phi)\n",
    "    weight_array[:,1]=factor*np.sin(phi)\n",
    "    bias_array=np.full(n_lines,factor*(+0.5-shift*np.cos(phi)))\n",
    "    full_weight_array[j*n_lines:(j+1)*n_lines,:]=weight_array[:,:]\n",
    "    full_bias_array[j*n_lines:(j+1)*n_lines]=bias_array[:]\n",
    "    j+=1\n",
    "\n",
    "# sharper sigmoid: scale all weights and biases!\n",
    "factor=20.0\n",
    "# this needs np.array(), because you cannot do factor*<python-list>\n",
    "visualize_network(weights=[ \n",
    "    full_weight_array, \n",
    "    np.full([1,n_blobs*n_lines],1.0)\n",
    "    ],\n",
    "    biases=[ \n",
    "        full_bias_array,\n",
    "        [0.0]\n",
    "    ],\n",
    "    activations=[ 'sigmoid', # activation for hidden\n",
    "                'linear' # activation for output\n",
    "                ],\n",
    "    y0range=[-3,3],y1range=[-3,3],\n",
    "                 size=30.0,M=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the AND function\n",
    "# this computes the AND function of the inputs,\n",
    "# in the sense that y1=+1,y2=+1 maps to +1, but\n",
    "# the other combinations (like y1=0,y2=+1 etc.) all map to 0\n",
    "\n",
    "visualize_network(weights=[ [ \n",
    "    [1.0,1.0]  # weights of 2 input neurons for single output neuron\n",
    "    ] ],\n",
    "    biases=[ \n",
    "        [-1.5] # bias for single output neuron\n",
    "            ],\n",
    "    activations=[ 'jump' # activation for output\n",
    "                ],\n",
    "    y0range=[-3,3],y1range=[-3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
