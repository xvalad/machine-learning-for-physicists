{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten digits recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loader for the MNIST image data was taken from Nielsen's online book,\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "See specifically the following link, for downloading the MNIST image data (we only need the mnist.pkl.gz package inside the 'data' subdirectory; store it inside the present directory of the notebook):\n",
    "https://github.com/mnielsen/neural-networks-and-deep-learning/archive/master.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import keras library. Also import some of the layers, so we do not need to\n",
    "# write things like \"layers.Dense\", but can just write \"Dense\" instead\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import GaussianDropout\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "\n",
    "# Import the numpy library for matrix manipulations etc.\n",
    "\n",
    "#from numpy import *\n",
    "from numpy import array, zeros, exp, random, dot, shape, transpose, reshape, meshgrid, linspace, sqrt\n",
    "\n",
    "# Set up the graphics by importing the matplotlib plotting library\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# Set up a random number generator with a fixed seed, so that\n",
    "# running this whole notebook repeatedly should always give\n",
    "# the same result (useful for debugging)\n",
    "rng = random.RandomState(23455)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "taken from Nielsen's online book:\n",
    "http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f,encoding='bytes')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    \n",
    "    global training_inputs, training_results\n",
    "    global validation_inputs, validation_results\n",
    "    global test_inputs, test_results\n",
    "    global num_samples, numpixels, num_test_samples\n",
    "    \n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    num_samples=len(tr_d[0])\n",
    "    training_inputs=zeros([num_samples,numpixels])\n",
    "    training_results=zeros([num_samples,10])    \n",
    "    for j in range(num_samples):\n",
    "        training_inputs[j,:] = reshape(tr_d[0][j], (numpixels))\n",
    "        training_results[j,:] = vectorized_result(tr_d[1][j])\n",
    "#    validation_inputs = [reshape(x, (numpixels)) for x in va_d[0]]\n",
    "#    validation_results = [vectorized_result(y) for y in va_d[1]]\n",
    "\n",
    "    num_test_samples=len(te_d[0])\n",
    "    test_inputs=zeros([num_test_samples,numpixels])\n",
    "    test_results=zeros([num_test_samples,10])    \n",
    "    for j in range(num_test_samples):\n",
    "        test_inputs[j,:] = reshape(te_d[0][j], (numpixels))\n",
    "        test_results[j,:] = vectorized_result(te_d[1][j])\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = zeros((10))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize the network\n",
    "def init_net():\n",
    "    global net, numpixels\n",
    "    net = Sequential()\n",
    "    # note: batch_input_shape is (batchsize,timesteps,data_dim)\n",
    "    net.add(Dense(30, input_shape=(numpixels,), activation='relu'))\n",
    "    net.add(Dense(10, activation='softmax'))\n",
    "    net.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1.0), metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize the network\n",
    "def init_net_large():\n",
    "    global net, numpixels\n",
    "    net = Sequential()\n",
    "    # note: batch_input_shape is (batchsize,timesteps,data_dim)\n",
    "    net.add(Dense(100, input_shape=(numpixels,), activation='relu'))\n",
    "    net.add(GaussianDropout(0.1))\n",
    "    net.add(Dense(50, activation='relu'))\n",
    "    net.add(Dense(10, activation='softmax'))\n",
    "    net.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1.0), metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def test_on(start,stop,dontprint=False):\n",
    "    global test_inputs, test_results\n",
    "    global net, predictions_probs, predictions, true_labels\n",
    "    \n",
    "    predictions_probs=net.predict_on_batch(test_inputs[start:stop,:])\n",
    "    predictions=argmax(predictions_probs,axis=1)\n",
    "    if dontprint==False:\n",
    "        print(\"Predictions: \", predictions)\n",
    "    true_labels=argmax(test_results[start:stop,:], axis=1)\n",
    "    if dontprint==False:\n",
    "        print(\"True labels: \", true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(which):\n",
    "    global test_inputs\n",
    "    \n",
    "    plt.imshow(reshape(test_inputs[which,:],[28,28]),interpolation='nearest', cmap='binary')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_array(which):\n",
    "    global test_inputs\n",
    "    \n",
    "    numcolumns=8\n",
    "    BigImage=zeros([28*numcolumns,28*numcolumns])\n",
    "    for j in range(len(which)):\n",
    "        x=(j%numcolumns)*28\n",
    "        y=int(j/numcolumns)*28\n",
    "        BigImage[x:x+28,y:y+28]=reshape(test_inputs[which[j],:],[28,28])\n",
    "    plt.imshow(BigImage,interpolation='nearest', cmap='binary')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mistakes(maxnum):\n",
    "    global test_inputs, rest_results, num_test_samples\n",
    "    global true_labels, predictions, predictions_probs\n",
    "    \n",
    "    test_on(0,num_test_samples,dontprint=True)\n",
    "    which=where(true_labels!=predictions)[0]\n",
    "    for j in which:\n",
    "        if j<maxnum:\n",
    "            display_image(j)\n",
    "            print(\"True \", true_labels[j], \" - Predicted \", predictions[j], \" with prob. \", predictions_probs[j,predictions[j]])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "numpixels=784\n",
    "load_data_wrapper() # load all the MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "shape(training_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "psi=training_inputs-sum(training_inputs,axis=0)/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(reshape(psi[4,:],[28,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "shape(psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_image_array(range(8*8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "shape(training_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "init_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batchsize=100\n",
    "batches=int(num_samples/batchsize)-1\n",
    "costs=zeros(batches)\n",
    "for j in range(batches):\n",
    "    costs[j]=net.train_on_batch(training_inputs[j*batchsize:(j+1)*batchsize,:], training_results[j*batchsize:(j+1)*batchsize,:])[0]\n",
    "plt.plot(costs,linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_on(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use the keras \"fit\" function to go through the whole data set many times ('epochs'), \n",
    "# and even set aside some validation samples\n",
    "history=net.fit(training_inputs,training_results,batch_size=100,\n",
    "                epochs=30,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# see which fraction of the test samples is classified incorrectly\n",
    "test_on(0,num_test_samples,dontprint=True)\n",
    "which=where(true_labels!=predictions)[0]\n",
    "print(len(which)/num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "plt.plot(history.history['categorical_accuracy'], linewidth=3)\n",
    "plt.plot(history.history['val_categorical_accuracy'], linewidth=3)\n",
    "plt.show()\n",
    "#fig.savefig(\"Handwritten_MNIST_Fig1_Accuracy.pdf\")\n",
    "#fig.savefig(\"Handwritten_MNIST_Fig2_AccuracyAndValidation.pdf\")\n",
    "#fig.savefig(\"Handwritten_MNIST_Fig4_100_50_DropOut_AccuracyAndValidation.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display_mistakes(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
