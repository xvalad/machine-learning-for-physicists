{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the training of Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 3, Tutorials\n",
    "\n",
    "See https://machine-learning-for-physicists.org and the current course website linked there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to:\n",
    "- visualize the training of neural networks using keras\n",
    "\n",
    "The networks have 2 input and 1 output neurons, but arbitrarily many hidden layers, and also you can choose the activation functions\n",
    "\n",
    "This is essentially an extension of the lecture-2 training notebook, but now using keras instead of pure python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports: numpy and matplotlib and keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras: Sequential is the neural-network class, Dense is\n",
    "# the standard network layer\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers # to choose more advanced optimizers like 'adam'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# for subplots within subplots:\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# for nice inset colorbars: (approach changed from lecture 1 'Visualization' notebook)\n",
    "from mpl_toolkits.axes_grid1.inset_locator import InsetPosition\n",
    "\n",
    "# for updating display \n",
    "# (very simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation and training routines\n",
    "# these are now just front-ends to keras!\n",
    "\n",
    "def apply_net(y_in): # one forward pass through the network\n",
    "    global Net    \n",
    "    return(Net.predict_on_batch(y_in))\n",
    "\n",
    "def apply_net_simple(y_in): # one forward pass through the network\n",
    "    return(apply_net(y_in))\n",
    "        \n",
    "def train_net(y_in,y_target): # one full training batch\n",
    "    # y_in is an array of size batchsize x (input-layer-size)\n",
    "    # y_target is an array of size batchsize x (output-layer-size)\n",
    "    global Net\n",
    "    \n",
    "    cost=Net.train_on_batch(y_in,y_target)[0]\n",
    "    return(cost)\n",
    "\n",
    "def init_layer_variables(weights,biases,activations,use_keras_init=False,eta=0.1,optimizer='sgd'):\n",
    "    global Weights, Biases, NumLayers, Activations\n",
    "    global LayerSizes, y_layer, df_layer, dw_layer, db_layer\n",
    "    global Net\n",
    "    \n",
    "    # store the main data in global variables\n",
    "    Weights=weights\n",
    "    Biases=biases\n",
    "    Activations=activations\n",
    "    NumLayers=len(Weights)\n",
    "\n",
    "    # keras activation names can be slightly different from what I used...\n",
    "    # also: 'jump' is not implemented here\n",
    "    KerasActivation={ \"sigmoid\":\"sigmoid\", \"reLU\":\"relu\", \"linear\":\"linear\" }\n",
    "\n",
    "    Net=Sequential() # a new network ('sequential' is the simplest structure: layer by layer)\n",
    "    \n",
    "    # now build up the network, layer by layer:\n",
    "    LayerSizes=[2]\n",
    "    for j in range(NumLayers):\n",
    "        LayerSizes.append(len(Biases[j]))\n",
    "        if use_keras_init: # use keras' random weight initialization approach\n",
    "            Net.add(Dense(len(Biases[j]), # number of neurons\n",
    "                          input_shape=(LayerSizes[j],), # size of previous layer\n",
    "                          activation=KerasActivation[activations[j]] # activation function\n",
    "                     ))            \n",
    "        else:\n",
    "            Net.add(Dense(len(Biases[j]), # number of neurons\n",
    "                          input_shape=(LayerSizes[j],), # size of previous layer\n",
    "                          activation=KerasActivation[activations[j]], # activation function\n",
    "                         weights = [ np.array(weights[j]), np.array(biases[j]) ] # the weights and biases for this layer\n",
    "                     ))\n",
    "    if optimizer=='adam':\n",
    "        the_optimizer=optimizers.Adam(lr=eta) # adaptive\n",
    "    else:\n",
    "        the_optimizer=optimizers.SGD(lr=eta) # standard gradient descent with given learning rate!\n",
    "    Net.compile(loss='mean_squared_error',\n",
    "              optimizer=the_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization routines:\n",
    "\n",
    "# some internal routines for plotting the network:\n",
    "def plot_connection_line(ax,X,Y,W,vmax=1.0,linewidth=3):\n",
    "    t=np.linspace(0,1,20)\n",
    "    if W>0:\n",
    "        col=[0,0.4,0.8]\n",
    "    else:\n",
    "        col=[1,0.3,0]\n",
    "    ax.plot(X[0]+(3*t**2-2*t**3)*(X[1]-X[0]),Y[0]+t*(Y[1]-Y[0]),\n",
    "           alpha=abs(W)/vmax,color=col,\n",
    "           linewidth=linewidth)\n",
    "    \n",
    "def plot_neuron_alpha(ax,X,Y,B,size=100.0,vmax=1.0):\n",
    "    if B>0:\n",
    "        col=np.array([[0,0.4,0.8]])\n",
    "    else:\n",
    "        col=np.array([[1,0.3,0]])\n",
    "    ax.scatter([X],[Y],marker='o',c=col,alpha=abs(B)/vmax,s=size,zorder=10)\n",
    "\n",
    "def plot_neuron(ax,X,Y,B,size=100.0,vmax=1.0):\n",
    "    if B>0:\n",
    "        col=np.array([[0,0.4,0.8]])\n",
    "    else:\n",
    "        col=np.array([[1,0.3,0]])\n",
    "    ax.scatter([X],[Y],marker='o',c=col,s=size,zorder=10)\n",
    "    \n",
    "def visualize_network(weights,biases,activations,\n",
    "                      M=100,y0range=[-1,1],y1range=[-1,1],\n",
    "                     size=400.0, linewidth=5.0,\n",
    "                     weights_are_swapped=False,\n",
    "                    layers_already_initialized=False,\n",
    "                      plot_cost_function=None,\n",
    "                      current_cost=None, cost_max=None, plot_target=None\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Visualize a neural network with 2 input \n",
    "    neurons and 1 output neuron (plot output vs input in a 2D plot)\n",
    "    \n",
    "    weights is a list of the weight matrices for the\n",
    "    layers, where weights[j] is the matrix for the connections\n",
    "    from layer j to layer j+1 (where j==0 is the input)\n",
    "    \n",
    "    weights[j][m,k] is the weight for input neuron k going to output neuron m\n",
    "    (note: internally, m and k are swapped, see the explanation of\n",
    "    batch processing in lecture 2)\n",
    "    \n",
    "    biases[j] is the vector of bias values for obtaining the neurons in layer j+1\n",
    "    biases[j][k] is the bias for neuron k in layer j+1\n",
    "    \n",
    "    activations is a list of the activation functions for\n",
    "    the different layers: choose 'linear','sigmoid',\n",
    "    'jump' (i.e. step-function), and 'reLU'\n",
    "    \n",
    "    M is the resolution (MxM grid)\n",
    "    \n",
    "    y0range is the range of y0 neuron values (horizontal axis)\n",
    "    y1range is the range of y1 neuron values (vertical axis)\n",
    "    \"\"\"\n",
    "    if not weights_are_swapped:\n",
    "        swapped_weights=[]\n",
    "        for j in range(len(weights)):\n",
    "            swapped_weights.append(np.transpose(weights[j]))\n",
    "    else:\n",
    "        swapped_weights=weights\n",
    "\n",
    "    y0,y1=np.meshgrid(np.linspace(y0range[0],y0range[1],M),np.linspace(y1range[0],y1range[1],M))\n",
    "    y_in=np.zeros([M*M,2])\n",
    "    y_in[:,0]=y0.flatten()\n",
    "    y_in[:,1]=y1.flatten()\n",
    "    \n",
    "    # if we call visualization directly, we still\n",
    "    # need to initialize the 'Weights' and other\n",
    "    # global variables; otherwise (during training)\n",
    "    # all of this has already been taken care of:\n",
    "    if not layers_already_initialized:\n",
    "        init_layer_variables(swapped_weights,biases,activations)\n",
    "    y_out=apply_net_simple(y_in)\n",
    "\n",
    "    if plot_cost_function is None:\n",
    "        fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(8,4))\n",
    "    else:\n",
    "        fig=plt.figure(figsize=(8,4))\n",
    "        gs_top = gridspec.GridSpec(nrows=1, ncols=2)\n",
    "        gs_left = gridspec.GridSpecFromSubplotSpec(nrows=2, ncols=1, subplot_spec=gs_top[0], height_ratios=[1.0,0.3])\n",
    "        ax=[ fig.add_subplot(gs_left[0]),\n",
    "            fig.add_subplot(gs_top[1]),\n",
    "           fig.add_subplot(gs_left[1]) ]\n",
    "        # ax[0] is network\n",
    "        # ax[1] is image produced by network\n",
    "        # ax[2] is cost function subplot\n",
    "        \n",
    "    # plot the network itself:\n",
    "    \n",
    "    # positions of neurons on plot:\n",
    "    posX=[[-0.5,+0.5]]; posY=[[0,0]]\n",
    "    vmax=0.0 # for finding the maximum weight\n",
    "    vmaxB=0.0 # for maximum bias\n",
    "    for j in range(len(biases)):\n",
    "        n_neurons=len(biases[j])\n",
    "        posX.append(np.array(range(n_neurons))-0.5*(n_neurons-1))\n",
    "        posY.append(np.full(n_neurons,j+1))\n",
    "        vmax=np.maximum(vmax,np.max(np.abs(weights[j])))\n",
    "        vmaxB=np.maximum(vmaxB,np.max(np.abs(biases[j])))\n",
    "\n",
    "    # plot connections\n",
    "    for j in range(len(biases)):\n",
    "        for k in range(len(posX[j])):\n",
    "            for m in range(len(posX[j+1])):\n",
    "                plot_connection_line(ax[0],[posX[j][k],posX[j+1][m]],\n",
    "                                     [posY[j][k],posY[j+1][m]],\n",
    "                                     swapped_weights[j][k,m],vmax=vmax,\n",
    "                                    linewidth=linewidth)\n",
    "    \n",
    "    # plot neurons\n",
    "    for k in range(len(posX[0])): # input neurons (have no bias!)\n",
    "        plot_neuron(ax[0],posX[0][k],posY[0][k],\n",
    "                   vmaxB,vmax=vmaxB,size=size)\n",
    "    for j in range(len(biases)): # all other neurons\n",
    "        for k in range(len(posX[j+1])):\n",
    "            plot_neuron(ax[0],posX[j+1][k],posY[j+1][k],\n",
    "                       biases[j][k],vmax=vmaxB,size=size)\n",
    "    \n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # now: the output of the network\n",
    "    img=ax[1].imshow(np.reshape(y_out,[M,M]),origin='lower',\n",
    "                    extent=[y0range[0],y0range[1],y1range[0],y1range[1]])\n",
    "    ax[1].set_xlabel(r'$y_0$')\n",
    "    ax[1].set_ylabel(r'$y_1$')\n",
    "    \n",
    "#     axins1 = inset_axes(ax[1],\n",
    "#                     width=\"40%\",  # width = 50% of parent_bbox width\n",
    "#                     height=\"5%\",  # height : 5%\n",
    "#                     loc='upper right',\n",
    "#                        bbox_to_anchor=[0.3,0.4])\n",
    "\n",
    "#    axins1 = ax[1].inset_axes([0.5,0.8,0.45,0.1])\n",
    "    axins1 = plt.axes([0, 0, 1, 1])\n",
    "    ip = InsetPosition(ax[1], [0.25, 0.1, 0.5, 0.05])\n",
    "    axins1.set_axes_locator(ip)\n",
    "\n",
    "    imgmin=np.min(y_out)\n",
    "    imgmax=np.max(y_out)\n",
    "    color_bar=fig.colorbar(img, cax=axins1, orientation=\"horizontal\",ticks=np.linspace(imgmin,imgmax,3))\n",
    "    cbxtick_obj = plt.getp(color_bar.ax.axes, 'xticklabels')\n",
    "    plt.setp(cbxtick_obj, color=\"white\")\n",
    "    axins1.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "    if plot_target is not None:\n",
    "        axins2 = plt.axes([0.01, 0.01, 0.99, 0.99])\n",
    "        ip = InsetPosition(ax[1], [0.75, 0.75, 0.2, 0.2])\n",
    "        axins2.set_axes_locator(ip)\n",
    "        axins2.imshow(plot_target,origin='lower')\n",
    "        axins2.get_xaxis().set_ticks([])\n",
    "        axins2.get_yaxis().set_ticks([])\n",
    "        \n",
    "    if plot_cost_function is not None:\n",
    "        ax[2].plot(plot_cost_function)\n",
    "        ax[2].set_ylim([0.0,cost_max])\n",
    "        ax[2].set_yticks([0.0,cost_max])\n",
    "        ax[2].set_yticklabels([\"0\",'{:1.2e}'.format(cost_max)])\n",
    "        if current_cost is not None:\n",
    "            ax[2].text(0.9, 0.9, 'cost={:1.2e}'.format(current_cost), horizontalalignment='right',\n",
    "                       verticalalignment='top', transform=ax[2].transAxes)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def visualize_network_training(weights,biases,activations,\n",
    "                               target_function,\n",
    "                               num_neurons=None,\n",
    "                               weight_scale=1.0,\n",
    "                               bias_scale=1.0,\n",
    "                               yspread=1.0,\n",
    "                      M=100,y0range=[-1,1],y1range=[-1,1],\n",
    "                     size=400.0, linewidth=5.0,\n",
    "                    steps=100, batchsize=10, eta=0.1,\n",
    "                              random_init=False,\n",
    "                              visualize_nsteps=1,\n",
    "                              plot_target=True,\n",
    "                              use_keras_init=False,\n",
    "                              optimizer='sgd'):\n",
    "    \"\"\"\n",
    "    Visualize the training of a neural network.\n",
    "    \n",
    "    weights, biases, and activations define the neural network \n",
    "    (the starting point of the optimization; for the detailed description,\n",
    "    see the help for visualize_network)\n",
    "    \n",
    "    If you want to have layers randomly initialized, just provide\n",
    "    the number of neurons for each layer as 'num_neurons'. This should include\n",
    "    all layers, including input (2 neurons) and output (1), so num_neurons=[2,3,5,4,1] is\n",
    "    a valid example. In this case, weight_scale and bias_scale define the\n",
    "    spread of the random Gaussian variables used to initialize all weights and biases.\n",
    "    \n",
    "    target_function is the name of the function that we\n",
    "    want to approximate; it must be possible to \n",
    "    evaluate this function on a batch of samples, by\n",
    "    calling target_function(y) on an array y of \n",
    "    shape [batchsize,2], where\n",
    "    the second index refers to the two coordinates\n",
    "    (input neuron values) y0 and y1. The return\n",
    "    value must be an array with one index, corresponding\n",
    "    to the batchsize. A valid example is:\n",
    "    \n",
    "    def my_target(y):\n",
    "        return( np.sin(y[:,0]) + np.cos(y[:,1]) )\n",
    "    \n",
    "    steps is the number of training steps\n",
    "    \n",
    "    batchsize is the number of samples per training step\n",
    "    \n",
    "    eta is the learning rate (stepsize in the gradient descent)\n",
    "    \n",
    "    yspread denotes the spread of the Gaussian\n",
    "    used to sample points in (y0,y1)-space\n",
    "    \n",
    "    visualize_n_steps>1 means skip some steps before\n",
    "    visualizing again (can speed up things)\n",
    "    \n",
    "    plot_target=True means do plot the target function in a corner\n",
    "    \n",
    "    For all the other parameters, see the help for\n",
    "        visualize_network\n",
    "    \n",
    "    weights and biases as given here will be used\n",
    "    as starting points, unless you specify\n",
    "    random_init=True, in which case they will be\n",
    "    used to determine the spread of Gaussian random\n",
    "    variables used for initialization!\n",
    "    \"\"\"\n",
    "    global Net, Weights, Biases\n",
    "    \n",
    "    if num_neurons is not None: # build weight matrices as randomly initialized\n",
    "        weights=[weight_scale*np.random.randn(num_neurons[j+1],num_neurons[j]) for j in range(len(num_neurons)-1)]\n",
    "        biases=[bias_scale*np.random.randn(num_neurons[j+1]) for j in range(len(num_neurons)-1)]\n",
    "    \n",
    "    swapped_weights=[]\n",
    "    for j in range(len(weights)):\n",
    "        swapped_weights.append(np.transpose(weights[j]))\n",
    "    init_layer_variables(swapped_weights,biases,activations,use_keras_init=use_keras_init,eta=eta,optimizer=optimizer)\n",
    "    \n",
    "    if plot_target:\n",
    "        y0,y1=np.meshgrid(np.linspace(y0range[0],y0range[1],M),np.linspace(y1range[0],y1range[1],M))\n",
    "        y=np.zeros([M*M,2])\n",
    "        y[:,0]=y0.flatten()\n",
    "        y[:,1]=y1.flatten()\n",
    "        plot_target_values=np.reshape(target_function(y),[M,M])\n",
    "    else:\n",
    "        plot_target_values=None\n",
    "    \n",
    "    y_target=np.zeros([batchsize,1])\n",
    "    costs=np.zeros(steps)\n",
    "    \n",
    "    for j in range(steps):\n",
    "        # produce samples (random points in y0,y1-space):\n",
    "        y_in=yspread*np.random.randn(batchsize,2)\n",
    "        # apply target function to those points:\n",
    "        y_target[:,0]=target_function(y_in)\n",
    "        # do one training step on this batch of samples:\n",
    "        costs[j]=train_net(y_in,y_target)\n",
    "        \n",
    "        # now visualize the updated network:\n",
    "        if j%visualize_nsteps==0:\n",
    "            clear_output(wait=True) # for animation\n",
    "            if j>10:\n",
    "                cost_max=np.average(costs[0:j])*1.5\n",
    "            else:\n",
    "                cost_max=costs[0]\n",
    "\n",
    "            # extract weights and biases from the keras network:\n",
    "            for j in range(NumLayers):\n",
    "                Weights[j],Biases[j]=Net.layers[j].get_weights()\n",
    "            \n",
    "            visualize_network(Weights,Biases,activations,\n",
    "                          M,y0range=y0range,y1range=y1range,\n",
    "                         size=size, linewidth=linewidth,\n",
    "                             weights_are_swapped=True,\n",
    "                             layers_already_initialized=True,\n",
    "                             plot_cost_function=costs,\n",
    "                             current_cost=costs[j],\n",
    "                             cost_max=cost_max,\n",
    "                             plot_target=plot_target_values)\n",
    "            sleep(0.1) # wait a bit before next step (probably not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Training for a simple AND function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_target(y):\n",
    "    return( 1.0*( (y[:,0]+y[:,1])>0) )\n",
    "\n",
    "visualize_network_training(weights=[ [ \n",
    "    [0.2,-0.9]  # weights of 2 input neurons for single output neuron\n",
    "    ] ],\n",
    "    biases=[ \n",
    "        [0.0] # bias for single output neuron\n",
    "            ],\n",
    "    target_function=my_target, # the target function to approximate\n",
    "    activations=[ 'sigmoid' # activation for output\n",
    "                ],\n",
    "    y0range=[-3,3],y1range=[-3,3],\n",
    "    steps=1000, eta=.5, batchsize=200,\n",
    "                          visualize_nsteps=10, \n",
    "                           plot_target=True,\n",
    "                          optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Training for half a smiley (circle with two eyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def my_target(y):\n",
    "    a=0.8; r=0.5; R=2.0\n",
    "    return( 1.0*( y[:,0]**2+y[:,1]**2<R**2 ) - 1.0*( (y[:,0]-a)**2+(y[:,1]-a)**2<r**2) - 1.0*( (y[:,0]+a)**2+(y[:,1]-a)**2<r**2 ) )\n",
    "\n",
    "\n",
    "visualize_network_training(weights=[],biases=[],num_neurons=[2,30,30,1],\n",
    "    target_function=my_target, # the target function to approximate\n",
    "    activations=[ 'reLU','reLU','linear' ],\n",
    "    y0range=[-3,3],y1range=[-3,3],\n",
    "    steps=5000, eta=.1, batchsize=200,\n",
    "                          visualize_nsteps=100, \n",
    "                           plot_target=True,\n",
    "                          optimizer='adam',\n",
    "                          size=20,linewidth=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Extend this code so as to be able to use more advanced keras activation functions for the layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
