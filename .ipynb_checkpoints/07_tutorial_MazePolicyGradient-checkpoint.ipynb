{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Using Policy Gradient to solve a Maze\n",
    "\n",
    "This does not even use a neural network, but instead the policy is stored in an array (table-based policy gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 7, Tutorial (this is discussed in session 7)\n",
    "\n",
    "See https://machine-learning-for-physicists.org and the current course website linked there!\n",
    "\n",
    "This notebook is distributed under the Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license:\n",
    "\n",
    "https://creativecommons.org/licenses/by-sa/4.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to:\n",
    "- use policy gradient reinforcement learning to solve a given (fixed) maze (policy gradient based on a table, i.e. state space is sufficiently small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little robot is sent through a maze, to collect treasure chests. The maze and the distribution of treasure chests are fixed (i.e. in every trial the robot encounters the same maze). \n",
    "\n",
    "Will it be able to find the best strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# for subplots within subplots:\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze generation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maze generation algorithm from wikipedia\n",
    "# the code was removed in January 2020, but you can still\n",
    "# access it under this link:\n",
    "# https://en.wikipedia.org/w/index.php?title=Maze_generation_algorithm&oldid=930153705\n",
    "\n",
    "def maze(width=81, height=51, complexity=.75, density=.75):\n",
    "    # Only odd shapes\n",
    "    shape = ((height // 2) * 2 + 1, (width // 2) * 2 + 1)\n",
    "    # Adjust complexity and density relative to maze size\n",
    "    complexity = int(complexity * (5 * (shape[0] + shape[1]))) # number of components\n",
    "    density    = int(density * ((shape[0] // 2) * (shape[1] // 2))) # size of components\n",
    "    # Build actual maze\n",
    "    Z = np.zeros(shape, dtype=bool)\n",
    "    # Fill borders\n",
    "    Z[0, :] = Z[-1, :] = 1\n",
    "    Z[:, 0] = Z[:, -1] = 1\n",
    "    # Make aisles\n",
    "    for i in range(density):\n",
    "        x, y = np.random.randint(0, shape[1] // 2) * 2, np.random.randint(0, shape[0] // 2) * 2 # pick a random position\n",
    "        Z[y, x] = 1\n",
    "        for j in range(complexity):\n",
    "            neighbours = []\n",
    "            if x > 1:             neighbours.append((y, x - 2))\n",
    "            if x < shape[1] - 2:  neighbours.append((y, x + 2))\n",
    "            if y > 1:             neighbours.append((y - 2, x))\n",
    "            if y < shape[0] - 2:  neighbours.append((y + 2, x))\n",
    "            if len(neighbours):\n",
    "                y_,x_ = neighbours[np.random.randint(0, len(neighbours) - 1)]\n",
    "                if Z[y_, x_] == 0:\n",
    "                    Z[y_, x_] = 1\n",
    "                    Z[y_ + (y - y_) // 2, x_ + (x - x_) // 2] = 1\n",
    "                    x, y = x_, y_\n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze(width=5,height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(maze(width=35,height=35))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a little random walk inside the maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a maze, convert array from True/False to integer:\n",
    "world=np.array(maze(width=11,height=11),dtype='int')\n",
    "\n",
    "# the four directions of motion [delta_jx,delta_jy]:\n",
    "directions=np.array([[0,1],[0,-1],[1,0],[-1,0]])\n",
    "\n",
    "# initial position\n",
    "jx,jy=1,1\n",
    "\n",
    "# take a random walk through the maze and animate this!\n",
    "nsteps=40\n",
    "for j in range(nsteps):\n",
    "    # make a random step\n",
    "    pick=np.random.randint(4)\n",
    "    jx_new,jy_new=np.array([jx,jy])+directions[pick]\n",
    "    if world[jx_new,jy_new]==0: # is empty, can move!\n",
    "        jx,jy=jx_new,jy_new\n",
    "        # show what's happening\n",
    "        picture=np.copy(world) # copy the array (!)\n",
    "        picture[jx,jy]=2\n",
    "        plt.imshow(picture,origin='lower')\n",
    "        plt.show()\n",
    "        sleep(0.01)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now: policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\delta\\theta = \\eta R \\sum_{s,a} {\\partial \\over \\partial \\theta} \\ln \\pi_{\\theta}(s,a)$$\n",
    "\n",
    "Here $\\theta$ stands for the parameters controlling the policy probabilities $\\pi_{\\theta}(s,a)$, s is the state, a the action, and the sum runs over all state-action pairs that were encountered in the given trajectory, which led to an overall return (sum of rewards) R.\n",
    "\n",
    "Let's do a softmax-type parametrization:\n",
    "\n",
    "So: $$\\theta=(z_0,z_1,z_2,z_3)$$ and\n",
    "\n",
    "$$\\pi_{\\theta}(s,a=j) = {e^{z_j} \\over \\sum_k e^{z_k}} $$\n",
    "\n",
    "...where in reality the $z_j$ of course also depend on the state s, but for brevity we did not display that dependence here. They will all be stored in arrays, of the size of the maze!\n",
    "\n",
    "So, we get:\n",
    "\n",
    "$$ {\\partial \\over \\partial z_l} \\ln \\pi_{\\theta}(s,a=j) = \\delta_{l,j}-\\pi_{\\theta}(s,a=l) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum([0.1,0.3,0.4,0.2]) # cumulative sum, will be useful!\n",
    "# this is useful for drawing integer random numbers according\n",
    "# to a given arbitrary probability distribution!\n",
    "# just draw a uniformly distributed number p between 0 and 1 and then\n",
    "# check successively whether p<(entry of cumsum); stop when\n",
    "# this is first fulfilled! The index of the entry where you\n",
    "# stopped will be randomly distributed according to the\n",
    "# distribution given in the original array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The policy gradient algorithm\n",
    "\n",
    "In 124 lines of pure python code, including visualization and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full policy gradient RL for picking up 'treasure chests'\n",
    "# in an automatically generated maze; 2020 by F.M.\n",
    "\n",
    "M=11 # the size of the world: M x M\n",
    "eta=0.01 # the learning rate\n",
    "num_chests=10 # maximum number of treasure chests\n",
    "\n",
    "# make a maze, convert array from True/False to integer:\n",
    "world=np.array(maze(width=M,height=M),dtype='int')\n",
    "\n",
    "# the four directions of motion [delta_jx,delta_jy]:\n",
    "directions=np.array([[0,1],[0,-1],[1,0],[-1,0]])\n",
    "\n",
    "# the policy probabilities as an array policy[state,action]\n",
    "# here the state is represented by two coordinates jx,jy\n",
    "policy=np.full([M,M,4],0.25)\n",
    "# and also the underlying 'z'-values for the\n",
    "# softmax parametrization:\n",
    "policy_z=np.full([M,M,4],0.0)\n",
    "\n",
    "# steps inside one trajectory\n",
    "nsteps=80\n",
    "\n",
    "# total number of trials, i.e. trajectories\n",
    "ntrials=100\n",
    "skipsteps=5 # don't plot every trial\n",
    "\n",
    "# storing all the state/action pairs of the current trajectory\n",
    "states=np.zeros([nsteps,2], dtype='int')\n",
    "actions=np.zeros(nsteps, dtype='int')\n",
    "\n",
    "# a map of rewards (the 'boxes' are here!)\n",
    "reward=np.zeros([M,M])\n",
    "\n",
    "# storing all the returns, for all trials:\n",
    "Returns=np.zeros(ntrials)\n",
    "\n",
    "# try random selection of reward sites (treasure chests)\n",
    "for n in range(10): \n",
    "    jx_target,jy_target=np.random.randint(M,size=2)\n",
    "    if jx_target>4 or jy_target>4: # stay away from starting point!\n",
    "        if world[jx_target,jy_target]==0: # empty, keep it!\n",
    "            reward[jx_target,jy_target]+=1\n",
    "        \n",
    "# try many trajectories:\n",
    "for trial in range(ntrials):\n",
    "\n",
    "    # set return to zero for this trajectory:\n",
    "    R=0\n",
    "    # initial position:\n",
    "    jx,jy=1,1\n",
    "    \n",
    "    # go through all time steps\n",
    "    for t in range(nsteps):\n",
    "        # make a random step, according to the policy distribution\n",
    "        p=np.random.uniform()\n",
    "        cumulative_distribution=np.cumsum(policy[jx,jy,:])\n",
    "        for pick in range(4):\n",
    "            if p<cumulative_distribution[pick]:\n",
    "                break\n",
    "\n",
    "        # record the move\n",
    "        states[t,0]=jx\n",
    "        states[t,1]=jy\n",
    "        actions[t]=pick\n",
    "\n",
    "        # now make the move\n",
    "        jx_new,jy_new=np.array([jx,jy])+directions[pick]\n",
    "\n",
    "        # really make it if there is no wall:\n",
    "        if world[jx_new,jy_new]==0: # is empty, can move!\n",
    "            jx,jy=jx_new,jy_new\n",
    "        \n",
    "        # get a reward if on a treasure chest!\n",
    "        r=reward[jx,jy]\n",
    "        R+=r\n",
    "   \n",
    "    # store the return\n",
    "    Returns[trial]=R\n",
    "    \n",
    "    # use policy gradient update rule to adjust\n",
    "    # probabilities!\n",
    "    for t in range(nsteps): # go through the trajectory again\n",
    "        a=actions[t] # remember the action taken at step t\n",
    "        sx=states[t,0] # state/x-position at step\n",
    "        sy=states[t,1] # state/y-position\n",
    "        kronecker=np.zeros(4); kronecker[a]=1.0\n",
    "        policy_z[sx,sy,:]+=eta*R*(kronecker-policy[sx,sy,:])\n",
    "\n",
    "    # now calculate (again) the policy probab. from the z-values\n",
    "    policy=np.exp(policy_z)\n",
    "    policy/=np.sum(policy,axis=2)[:,:,None] # normalize\n",
    "    # these two steps together implement softmax on every\n",
    "    # site! efficient array syntax!\n",
    "    \n",
    "    # visualize!\n",
    "    if trial%skipsteps==0 or trial==ntrials-1:\n",
    "        # show what's happened in this trajectory\n",
    "        clear_output(wait=True)\n",
    "        fig,ax=plt.subplots(ncols=2,nrows=2,figsize=(7,7))\n",
    "        ax[0,0].plot(Returns) # all the returns, in all trials\n",
    "        ax[0,0].set_title(\"Return vs. trial\")\n",
    "        \n",
    "        picture=np.zeros([M,M,3]) # last index: red/green/blue\n",
    "        picture[:,:,0]=world # walls are red\n",
    "        for j in range(nsteps): # highlight trajectory\n",
    "            picture[states[j,0],states[j,1],1]=0.5*(1.0+(1.0*j)/nsteps)\n",
    "        # put a bright pixel at the positions visited\n",
    "        picture[:,:,2]+=0.5*reward # highlight the target sites!\n",
    "        picture[:,:,0]+=0.5*reward\n",
    "\n",
    "        # show picture (transpose is needed because\n",
    "        # otherwise the first coordinate jx is plotted upwards,\n",
    "        # not to the right)\n",
    "        ax[0,1].imshow(np.transpose(picture,[1,0,2]),origin='lower')\n",
    "        ax[0,1].axis('off')\n",
    "        \n",
    "        ax[1,0].imshow(np.transpose(policy[:,:,0]),origin='lower')\n",
    "        ax[1,0].axis('off')\n",
    "        ax[1,0].set_title(\"prob(move up)\")\n",
    "        ax[1,1].imshow(np.transpose(policy[:,:,2]),origin='lower')\n",
    "        ax[1,1].set_title(\"prob(move right)\")\n",
    "        ax[1,1].axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Exercise: Change the algorithm, such that the treasure chests are 'taken away' by the robot\n",
    "\n",
    "That is: the reward value at a given site should decrease every time the reward is received (but you need to re-initialize to the old reward map for every new trial!)\n",
    "\n",
    "Observe how the strategy looks like now!\n",
    "\n",
    "Attention: don't go to negative rewards! (unless you want to train the robot to avoid places after it picked up a chest there!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Exercise: Think about other rules of the game!\n",
    "\n",
    "The only constraint is that the best (or at least a good) strategy can be formulated based only on the position (e.g. wandering ghosts like in PacMan would be a problem for the robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Exercise: Invent a game where the state needs to contain additional information (rather than just the position jx,jy)!\n",
    "\n",
    "...but in such a way that it can still be handled with a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
